{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the AI Agent Workshop!","text":"<p>This workshop provides a hands-on introduction to the concepts, patterns, and practical implementations of AI agents, using the <code>agentic-playground</code> repository as a basis.</p> <p>Over the next three days, we will explore:</p> <ul> <li>Day 1: Foundations: Basic LLM interaction, handling multimodal inputs (images), and working with complex data structures like knowledge graphs.</li> <li>Day 2: Single Agents &amp; Orchestration: Equipping agents with tools, implementing autonomous agents using the ReAct pattern, and incorporating human-in-the-loop workflows.</li> <li>Day 3: Advanced Multi-Agent Systems: Designing systems where multiple specialized agents collaborate using structured workflows (LangGraph) and dynamic group chats (AutoGen/MagenticOne).</li> </ul> <p>Please use the navigation menu to access the content for each day, including module explanations, hands-on exercises, and solutions.</p> <p>Let's begin exploring the exciting world of AI agents!</p>"},{"location":"code/","title":"Code Examples","text":"<p>This section provides links to the original source code files within the <code>agentic-playground</code> repository that are referenced throughout the workshop modules.</p> <p>Note: It is recommended to browse the code directly in your cloned repository or on GitHub for the full context and latest versions.</p>"},{"location":"code/#day-1","title":"Day 1","text":"<ul> <li>Module 1: Basics (<code>src/01-basics/</code>)<ul> <li>hello-world.py</li> <li>streaming-output.py</li> <li>tool-calling.py</li> </ul> </li> <li>Module 2: Multimodal Models (<code>src/02-multimodal-models/</code>)<ul> <li>inspect-image.py</li> <li>compare-images.py</li> <li>voice-agent.py</li> </ul> </li> <li>Module 3: Complex Data (<code>src/03-complex-data/</code>)<ul> <li>knowledge-graphs.py</li> <li>create_onthologies.py</li> <li>use-onthology.py</li> <li>parse_invoice.py</li> </ul> </li> </ul>"},{"location":"code/#day-2","title":"Day 2","text":"<ul> <li>Module 4: Complex Problems (<code>src/04-complex-problems/</code>)<ul> <li>trucking-plan.py</li> <li>trucking-execute.py</li> <li>process-step.py</li> <li>do-research.py</li> <li>apply-for-job.py</li> <li>browser-use.py</li> <li>find-contract.py</li> </ul> </li> <li>Module 5: Single Agent (<code>src/05-single-agent/</code>)<ul> <li>plugins.py</li> <li>react-agent-lc.py</li> <li>react-agent-li.py</li> <li>reasoning-agent-sk.py</li> </ul> </li> <li>Module 6: Human-in-the-Loop (<code>src/06-human-in-the-loop/</code>)<ul> <li>interrupt.py</li> <li>report-agents.py</li> </ul> </li> </ul>"},{"location":"code/#day-3","title":"Day 3","text":"<ul> <li>Module 7: Multi-Agent Collaboration (<code>src/07-multi-agent-collaboration/</code>)<ul> <li>coding-agents.py</li> <li>reasoning-coder.py</li> </ul> </li> <li>Module 8: Society of Agents (<code>src/08-society-of-agents/</code>)<ul> <li>simple-group.py</li> <li>chef-and-group.py</li> <li>o1-with-chef-group.py</li> </ul> </li> </ul>"},{"location":"day1/module1/basics/","title":"Day 1 - Module 1: Basic LLM Interaction &amp; Tool Calling","text":"<p>Objective: Understand how to interact with Large Language Models (LLMs) programmatically, handle streaming responses, and enable models to use external tools.</p> <p>Source Code: <code>src/01-basics/</code></p>"},{"location":"day1/module1/basics/#introduction","title":"Introduction","text":"<p>At the heart of AI agents lies the ability to interact with powerful Large Language Models (LLMs). These models can understand and generate human-like text, answer questions, translate languages, and much more. In this first module, we will explore the fundamental ways to communicate with an LLM using the OpenAI API pattern (as used by the GitHub Models endpoint in this repository) and introduce the concept of \"tool calling,\" which allows the LLM to interact with external systems or functions.</p> <p>We will cover:</p> <ol> <li>Basic API Calls: Sending a prompt to the model and receiving a complete response.</li> <li>Streaming Responses: Receiving the model's response incrementally as it's generated.</li> <li>Tool Calling: Defining functions (tools) that the model can request to use to gather information or perform actions.</li> </ol> <p>Core Concept: LLM Interaction</p> <p>Understanding how to structure API calls, manage conversation history (messages), and interpret responses is foundational for building any LLM-powered application.</p>"},{"location":"day1/module1/basics/#setup-review","title":"Setup Review","text":"<p>Prerequisites</p> <p>Before running the examples, ensure you have:</p> <ol> <li>Cloned the <code>agentic-playground</code> repository.</li> <li>Installed the required Python packages (<code>pip install -r requirements.txt</code>).</li> <li>Created a <code>.env</code> file in the repository root with your <code>GITHUB_TOKEN</code> (a Personal Access Token with no specific permissions needed for GitHub Models inference).</li> </ol> <pre><code># .env file content\nGITHUB_TOKEN=\"your_github_pat_here\"\n</code></pre>"},{"location":"day1/module1/basics/#1-hello-world-basic-api-interaction","title":"1. Hello World: Basic API Interaction","text":"<p>File: <code>src/01-basics/hello-world.py</code></p> <p>This script demonstrates the simplest form of interaction: sending a message to the LLM and getting a single, complete response back.</p> <p>Code Breakdown:</p> <ul> <li>Import necessary libraries: <code>os</code> for environment variables, <code>OpenAI</code> for the client, <code>load_dotenv</code> to load the <code>.env</code> file.</li> </ul> <pre><code>import os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\n</code></pre> <ul> <li>Initialize the OpenAI Client:<ul> <li><code>base_url</code>: Points to the GitHub Models inference endpoint.</li> <li><code>api_key</code>: Reads the <code>GITHUB_TOKEN</code> from your environment variables (loaded from <code>.env</code>).</li> </ul> </li> </ul> <pre><code>client = OpenAI(\n    base_url=\"https://models.inference.ai.azure.com\",\n    api_key=os.environ[\"GITHUB_TOKEN\"],\n)\n</code></pre> <ul> <li>Define the Conversation:<ul> <li>Messages are provided as a list of dictionaries, each with a <code>role</code> (<code>system</code>, <code>user</code>, or <code>assistant</code>) and <code>content</code>.</li> <li>The <code>system</code> message sets the context or instructions for the model (e.g., \"antworte alles in franz\u00f6sisch\" - answer everything in French).</li> <li>The <code>user</code> message contains the user's query.</li> </ul> </li> </ul> <pre><code>messages=[\n    {\n        \"role\": \"system\",\n        \"content\": \"antworte alles in franz\u00f6sisch\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the capital of France?\",\n    }\n]\n</code></pre> <ul> <li>Call the Chat Completions API:<ul> <li><code>client.chat.completions.create()</code> sends the request.</li> <li><code>messages</code>: The conversation history/prompt.</li> <li><code>model</code>: Specifies the model to use (e.g., <code>gpt-4o-mini</code>).</li> <li><code>temperature</code>, <code>max_tokens</code>, <code>top_p</code>: Control the creativity, length, and sampling strategy of the response.</li> </ul> </li> </ul> <pre><code>response = client.chat.completions.create(\n    messages=messages,\n    model=\"gpt-4o-mini\",\n    temperature=1,\n    max_tokens=4096,\n    top_p=1\n)\n</code></pre> <ul> <li>Print the Response:<ul> <li>The model's reply is found within the <code>response</code> object.</li> </ul> </li> </ul> <pre><code>print(response.choices[0].message.content)\n</code></pre> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/01-basics\npython hello-world.py\n</code></pre> <p>You should see the answer to \"What is the capital of France?\" printed in French.</p>"},{"location":"day1/module1/basics/#2-streaming-output","title":"2. Streaming Output","text":"<p>File: <code>src/01-basics/streaming-output.py</code></p> <p>Waiting for the entire response can take time, especially for longer answers. Streaming allows you to receive the response piece by piece, improving the perceived responsiveness of the interaction.</p> <p>Code Breakdown:</p> <ul> <li>Client Setup: Similar to <code>hello-world.py</code>.</li> <li>API Call with Streaming:<ul> <li>The key difference is <code>stream=True</code>.</li> <li><code>stream_options={'include_usage': True}</code> optionally requests token usage information at the end.</li> </ul> </li> </ul> <pre><code>response = client.chat.completions.create(\n    messages=[\n        # ... (system and user messages) ...\n    ],\n    model=model_name,\n    stream=True,\n    stream_options={'include_usage': True}\n)\n</code></pre> <ul> <li>Processing the Stream:<ul> <li>The <code>response</code> object is now an iterator.</li> <li>We loop through each <code>update</code> in the stream.</li> <li>Each <code>update</code> can contain a small piece of the response text (<code>delta.content</code>). We print these pieces immediately.</li> <li>If usage information is included, it appears in a final update.</li> </ul> </li> </ul> <pre><code>usage = None\nfor update in response:\n    if update.choices and update.choices[0].delta:\n        print(update.choices[0].delta.content or \"\", end=\"\") # Print chunk without newline\n    if update.usage:\n        usage = update.usage\n\nif usage:\n    print(\"\\n\") # Add newline after full response\n    for k, v in usage.model_dump().items():\n        print(f\"{k} = {v}\")\n</code></pre> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/01-basics\npython streaming-output.py\n</code></pre> <p>You will see the reasons for exercising appear on the console incrementally, followed by the token usage statistics.</p>"},{"location":"day1/module1/basics/#3-tool-calling-extending-llm-capabilities","title":"3. Tool Calling: Extending LLM Capabilities","text":"<p>File: <code>src/01-basics/tool-calling.py</code></p> <p>LLMs are trained on vast datasets but lack real-time information and the ability to perform actions in the real world. Tool calling allows the LLM to request the execution of predefined functions (tools) to overcome these limitations.</p> <p>Concept:</p> <ol> <li>Define Tools: You describe available functions (like getting the current time, searching the web, etc.) to the LLM, including their names, descriptions, and expected parameters.</li> <li>LLM Request: When the LLM determines it needs a tool to answer a user's query, it doesn't directly answer but instead outputs a special message indicating which tool to call and with what arguments.</li> <li>Execute Tool: Your code receives this request, executes the corresponding function with the provided arguments.</li> <li>Provide Result: You send the function's return value back to the LLM.</li> <li>Final Response: The LLM uses the tool's result to formulate the final answer to the user.</li> </ol> <p>Why Tool Calling is Powerful</p> <p>Tool calling transforms LLMs from passive text generators into active agents capable of interacting with APIs, databases, or custom code, vastly expanding their potential applications.</p> <p>Code Breakdown:</p> <ul> <li>Import additional libraries: <code>json</code> for parsing arguments, <code>pytz</code> and <code>datetime</code> for the time function.</li> <li>Define the Tool Function: A standard Python function (<code>get_current_time</code>) that takes a city name and returns the time. Note the docstring, which helps the LLM understand what the function does.</li> </ul> <pre><code>import pytz\nfrom datetime import datetime\n\ndef get_current_time(city_name: str) -&gt; str:\n    \"\"\"Returns the current time in a given city.\"\"\"\n    # ... (implementation using pytz) ...\n</code></pre> <ul> <li>Define the Tool Schema: A dictionary describing the function to the LLM.<ul> <li><code>type</code>: Always \"function\".</li> <li><code>function</code>: Contains details:<ul> <li><code>name</code>: Must match the Python function name.</li> <li><code>description</code>: Crucial for the LLM to know when to use the tool.</li> <li><code>parameters</code>: Describes the arguments (name, type, description, required).</li> </ul> </li> </ul> </li> </ul> <pre><code>tool={\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_time\",\n        \"description\": \"\"\"Returns information about the current time...\"\"\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"city_name\": {\n                    \"type\": \"string\",\n                    \"description\": \"The name of the city...\",\n                }\n            },\n            \"required\": [\"city_name\"],\n        },\n    },\n}\n</code></pre> <p>Designing Good Tool Descriptions</p> <p>The <code>description</code> field in the tool schema is critical. It should clearly and concisely explain what the tool does and when it should be used. Use natural language that the LLM can easily understand.</p> <ul> <li>Initial API Call with Tools:<ul> <li>The <code>tools</code> parameter is added to the <code>create</code> call, listing the available tools.</li> </ul> </li> </ul> <pre><code>response = client.chat.completions.create(\n    messages=messages,\n    tools=[tool], # Pass the tool definition\n    model=model_name,\n)\n</code></pre> <ul> <li>Handling Tool Call Response:<ul> <li>Check if <code>finish_reason</code> is <code>tool_calls</code>.</li> <li>Append the model's request message to the history.</li> <li>Extract the <code>tool_call</code> information (ID, function name, arguments).</li> <li>Parse the JSON arguments.</li> <li>Crucially, call the actual Python function (<code>locals()[tool_call.function.name](**function_args)</code>).</li> <li>Append the tool's result back to the message history, using the <code>tool_call_id</code> and <code>role: \"tool\"</code>.</li> </ul> </li> </ul> <pre><code>if response.choices[0].finish_reason == \"tool_calls\":\n    messages.append(response.choices[0].message) # Append assistant's request\n    tool_call = response.choices[0].message.tool_calls[0]\n    if tool_call.type == \"function\":\n        function_args = json.loads(tool_call.function.arguments)\n        callable_func = locals()[tool_call.function.name]\n        function_return = callable_func(**function_args)\n        messages.append( # Append tool result\n            {\n                \"tool_call_id\": tool_call.id,\n                \"role\": \"tool\",\n                \"name\": tool_call.function.name,\n                \"content\": function_return,\n            }\n        )\n</code></pre> <ul> <li>Second API Call: Call the model again with the updated message history (including the tool result).</li> </ul> <pre><code>response = client.chat.completions.create(\n    messages=messages,\n    tools=[tool],\n    model=model_name,\n)\nprint(f\"Model response = {response.choices[0].message.content}\")\n</code></pre> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/01-basics\n# Install pytz if you haven't: pip install pytz\npython tool-calling.py\n</code></pre> <p>You will see output indicating the function call (<code>Calling function 'get_current_time'...</code>), the function's return value, and finally the model's response incorporating the time information.</p> <p>Further Reading &amp; Resources</p> <p>To deepen your understanding of basic LLM interaction and tool calling, explore these resources:</p> <ul> <li>General LLM Interaction:<ul> <li>The Beginner's Guide to Language Models with Python (Machine Learning Mastery)</li> <li>Using Large Language Models APIs with Python (Medium)</li> <li>How to Build LLM Applications with LangChain Tutorial (DataCamp)</li> </ul> </li> <li>Tool Calling Specifics:<ul> <li>LangChain Documentation: Tool calling concepts</li> <li>LangChain Blog Post: Tool Calling with LangChain</li> <li>Analytics Vidhya Guide: Guide to Tool Calling in LLMs</li> <li>Medium Tutorial: Tool Calling for LLMs: A Detailed Tutorial</li> <li>Apideck Introduction: An introduction to function calling and tool use</li> <li>Mistral AI Docs: Function calling</li> <li>The Register Guide: A quick guide to tool-calling in large language models</li> </ul> </li> </ul> <p>This module covered the basics of interacting with LLMs and enabling them to use tools. In the next module, we'll explore how models can handle different types of input, specifically images and voice.</p>"},{"location":"day1/module1/exercises/","title":"Day 1 - Module 1: Exercises","text":"<p>These exercises are based on the concepts and code presented in Module 1: Basic LLM Interaction &amp; Tool Calling (<code>src/01-basics/</code>).</p>"},{"location":"day1/module1/exercises/#exercise-11-changing-language-and-persona","title":"Exercise 1.1: Changing Language and Persona","text":"<p>Goal: Practice modifying the system prompt to change the LLM's behavior.</p> <ol> <li>Open the <code>src/01-basics/hello-world.py</code> script.</li> <li>Modify the <code>system</code> message content from \"antworte alles in franz\u00f6sisch\" to instruct the model to respond like a pirate (e.g., \"Answer everything like a pirate.\").</li> <li>Modify the <code>user</code> message content to ask a different question (e.g., \"What is the weather like today?\").</li> <li>Run the script (<code>python hello-world.py</code>) and observe the pirate-themed response.</li> </ol>"},{"location":"day1/module1/exercises/#exercise-12-using-the-time-tool-for-a-different-city","title":"Exercise 1.2: Using the Time Tool for a Different City","text":"<p>Goal: Practice changing the user input to trigger a tool call with different arguments.</p> <ol> <li>Open the <code>src/01-basics/tool-calling.py</code> script.</li> <li>Locate the initial <code>user</code> message:     <pre><code>messages=[\n    {\n        \"role\": \"user\",\n        \"content\": \"What time is it in my current location?\",\n    }\n]\n</code></pre></li> <li>Change the <code>content</code> to ask for the time in a specific city, for example, Tokyo: \"What time is it in Tokyo?\".</li> <li>Run the script (<code>python tool-calling.py</code>).</li> <li>Observe the output. Does the <code>get_current_time</code> function get called with <code>Asia/Tokyo</code> (or a similar valid timezone identifier derived by the LLM)? Does the final model response include the correct time for Tokyo?</li> </ol>"},{"location":"day1/module1/exercises/#exercise-13-advanced-adding-a-simple-calculator-tool","title":"Exercise 1.3 (Advanced): Adding a Simple Calculator Tool","text":"<p>Goal: Practice defining and integrating a new tool for the LLM to use.</p> <p>Tool calling allows the LLM to interact with external functions or APIs to retrieve information or perform actions. The diagram below illustrates the concept:</p> <p></p> <ol> <li>Open the <code>src/01-basics/tool-calling.py</code> script.</li> <li>Define a new Python function: Add a function that performs simple addition.     <pre><code>def add_numbers(a: int, b: int) -&gt; int:\n    \\\"\\\"\\\"Adds two integer numbers together.\\\"\\\"\\\"\n    print(f\"Calling function 'add_numbers' with {a} and {b}\")\n    return a + b\n</code></pre></li> <li>Define the tool schema: Create a new dictionary describing the <code>add_numbers</code> function to the LLM, similar to the <code>get_current_time</code> schema.     <pre><code>add_tool = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"add_numbers\",\n        \"description\": \"Adds two integer numbers together.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"a\": {\n                    \"type\": \"integer\",\n                    \"description\": \"The first number.\",\n                },\n                \"b\": {\n                    \"type\": \"integer\",\n                    \"description\": \"The second number.\",\n                }\n            },\n            \"required\": [\"a\", \"b\"],\n        },\n    },\n}\n</code></pre></li> <li>Update the API calls:<ul> <li>Modify the <code>tools</code> list passed to <code>client.chat.completions.create</code> to include both the original time tool (<code>tool</code>) and the new <code>add_tool</code>:     <pre><code>tools=[tool, add_tool]\n</code></pre></li> <li>Ensure this updated <code>tools</code> list is used in both the initial API call and the second call made after handling a potential tool invocation.</li> </ul> </li> <li>Update the user query: Change the initial <code>user</code> message to ask an addition question, e.g., \"What is 123 plus 456?\".</li> <li>Run the script: <code>python3.11 tool-calling.py</code>.</li> <li>Observe the output. Does the LLM correctly identify the need for the <code>add_numbers</code> tool? Is the function called with the correct arguments? Does the final response contain the sum?</li> </ol>"},{"location":"day1/module1/solutions/","title":"Day 1 - Module 1: Solutions","text":"<p>These are the solutions for the exercises in Module 1.</p>"},{"location":"day1/module1/solutions/#solution-11-changing-language-and-persona","title":"Solution 1.1: Changing Language and Persona","text":"<p>Modify <code>src/01-basics/hello-world.py</code>:</p> <pre><code>import os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclient = OpenAI(\n    base_url=\"https://models.inference.ai.azure.com\",\n    api_key=os.environ[\"GITHUB_TOKEN\"],\n)\n\n# Modified messages\nmessages=[\n    {\n        \"role\": \"system\",\n        # Changed system message to request pirate persona\n        \"content\": \"Answer everything like a pirate.\", \n    },\n    {\n        \"role\": \"user\",\n        # Changed user question\n        \"content\": \"What is the weather like today?\", \n    }\n]\n\nresponse = client.chat.completions.create(\n    messages=messages,\n    model=\"gpt-4o-mini\",\n    temperature=1,\n    max_tokens=4096,\n    top_p=1\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <p>Expected Output: The model should respond to the weather question with pirate-like language (e.g., \"Ahoy! The weather be...\" etc.).</p>"},{"location":"day1/module1/solutions/#solution-12-using-the-time-tool-for-a-different-city","title":"Solution 1.2: Using the Time Tool for a Different City","text":"<p>Modify <code>src/01-basics/tool-calling.py</code>:</p> <pre><code># ... (imports and function definitions remain the same) ...\n\n# Initial user message asking for time in Tokyo\nmessages=[\n    {\n        \"role\": \"user\",\n        \"content\": \"What time is it in Tokyo?\", # Changed city\n    }\n]\n\n# ... (rest of the script remains the same) ...\n</code></pre> <p>Expected Output: *   The script should print <code>Calling function 'get_current_time' with city_name='Asia/Tokyo'</code> (or similar valid timezone). *   It should print the actual current time in Tokyo. *   The final model response should incorporate the time in Tokyo (e.g., \"The current time in Tokyo is [time].\").</p>"},{"location":"day1/module1/solutions/#solution-13-advanced-adding-a-simple-calculator-tool","title":"Solution 1.3 (Advanced): Adding a Simple Calculator Tool","text":"<p>Modify <code>src/01-basics/tool-calling.py</code>:</p> <pre><code>import os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport json\nimport pytz\nfrom datetime import datetime\n\nload_dotenv()\n\n# --- Add the new function --- \ndef add_numbers(a: int, b: int) -&gt; int:\n    \"\"\"Adds two integer numbers together.\"\"\"\n    print(f\"Calling function 'add_numbers' with {a} and {b}\")\n    return a + b\n# ---------------------------\n\ndef get_current_time(city_name: str) -&gt; str:\n    \"\"\"Returns the current time in a given city.\"\"\"\n    # ... (implementation remains the same)\n    try:\n        print(f\"Calling function 'get_current_time' with city_name='{city_name}'\")\n        timezone = pytz.timezone(city_name)\n        now = datetime.now(timezone)\n        current_time = now.strftime(\"%I:%M:%S %p\")\n        return current_time\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return \"Sorry, I couldn't find the timezone for that location.\"\n\nclient = OpenAI(\n    base_url=\"https://models.inference.ai.azure.com\",\n    api_key=os.environ[\"GITHUB_TOKEN\"],\n)\n\nmodel_name = \"gpt-4o-mini\"\n\n# --- Define the schema for the new tool --- \nadd_tool = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"add_numbers\",\n        \"description\": \"Adds two integer numbers together.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"a\": {\n                    \"type\": \"integer\",\n                    \"description\": \"The first number.\",\n                },\n                \"b\": {\n                    \"type\": \"integer\",\n                    \"description\": \"The second number.\",\n                }\n            },\n            \"required\": [\"a\", \"b\"],\n        },\n    },\n}\n# -----------------------------------------\n\ntool={\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_current_time\",\n        \"description\": \"\"\"Returns information about the current time in a given city. Use pytz timezone names like Europe/Berlin or Asia/Tokyo.\"\"\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"city_name\": {\n                    \"type\": \"string\",\n                    \"description\": \"The name of the city in pytz timezone format, e.g. Europe/Berlin\",\n                }\n            },\n            \"required\": [\"city_name\"],\n        },\n    },\n}\n\n# --- Update the user query --- \nmessages=[\n    {\n        \"role\": \"user\",\n        \"content\": \"What is 123 plus 456?\", \n    }\n]\n# ---------------------------\n\n# --- Update the tools list in BOTH API calls --- \nresponse = client.chat.completions.create(\n    messages=messages,\n    tools=[tool, add_tool], # Include both tools\n    model=model_name,\n)\n\nif response.choices[0].finish_reason == \"tool_calls\":\n    messages.append(response.choices[0].message)\n    tool_call = response.choices[0].message.tool_calls[0]\n    if tool_call.type == \"function\":\n        function_args = json.loads(tool_call.function.arguments)\n        # Ensure locals() can find the function\n        if tool_call.function.name in locals():\n            callable_func = locals()[tool_call.function.name]\n            function_return = callable_func(**function_args)\n            messages.append(\n                {\n                    \"tool_call_id\": tool_call.id,\n                    \"role\": \"tool\",\n                    \"name\": tool_call.function.name,\n                    \"content\": str(function_return), # Ensure content is string\n                }\n            )\n        else:\n             print(f\"Error: Function {tool_call.function.name} not found.\")\n             messages.append(\n                {\n                    \"tool_call_id\": tool_call.id,\n                    \"role\": \"tool\",\n                    \"name\": tool_call.function.name,\n                    \"content\": f\"Error: Function {tool_call.function.name} not found.\",\n                }\n            )\n\n    # --- Make sure the updated tools list is used here too --- \n    response = client.chat.completions.create(\n        messages=messages,\n        tools=[tool, add_tool], # Include both tools\n        model=model_name,\n    )\n\nprint(f\"Model response = {response.choices[0].message.content}\")\n# -------------------------------------------------------\n</code></pre> <p>Expected Output: *   The script should print <code>Calling function 'add_numbers' with 123 and 456</code>. *   The final model response should be something like \"123 plus 456 is 579.\".</p>"},{"location":"day1/module2/exercises/","title":"Day 1 - Module 2: Exercises","text":"<p>These exercises are based on the concepts and code presented in Module 2: Working with Multimodal Models (<code>src/02-multimodal-models/</code>).</p>"},{"location":"day1/module2/exercises/#exercise-21-describing-a-different-image","title":"Exercise 2.1: Describing a Different Image","text":"<p>Goal: Practice using the vision model to analyze a new image and answer a specific question.</p> <ol> <li>Find an image online (e.g., a picture of a cat, a landscape, a famous landmark). Save it to the <code>src/02-multimodal-models/</code> directory (e.g., as <code>my_image.jpg</code>).</li> <li>Open the <code>src/02-multimodal-models/inspect-image.py</code> script.</li> <li>Modify the script to use your new image file:<ul> <li>Update the filename in the <code>get_image_data_url</code> call (e.g., <code>get_image_data_url(\"my_image.jpg\", \"jpg\")</code>). Make sure the format (\"jpg\", \"png\", etc.) matches your image.</li> </ul> </li> <li>Modify the <code>text</code> part of the user prompt to ask a specific question about your image (e.g., \"What is the main animal in this image?\", \"What time of day does it look like in this picture?\", \"Describe the building in this image.\").</li> <li>Run the script (<code>python inspect-image.py</code>) and observe the model's description based on your question and image.</li> </ol>"},{"location":"day1/module2/exercises/#exercise-22-comparing-different-images-for-similarities","title":"Exercise 2.2: Comparing Different Images for Similarities","text":"<p>Goal: Practice using the vision model to compare two different images, focusing on similarities.</p> <ol> <li>Find two images online that have some similarities but also differences (e.g., two different types of dogs, two different models of cars, two different chairs). Save them to the <code>src/02-multimodal-models/</code> directory (e.g., <code>image_A.png</code>, <code>image_B.png</code>).</li> <li>Open the <code>src/02-multimodal-models/compare-images.py</code> script.</li> <li>Modify the script to use your two new images:<ul> <li>Update the filenames and formats in the two <code>get_image_data_url</code> calls.</li> </ul> </li> <li>Modify the <code>text</code> part of the user prompt to ask the model to focus on the similarities between the two images (e.g., \"Look at these two images. What features do they have in common?\").</li> <li>Run the script (<code>python compare-images.py</code>) and analyze the model's response highlighting the similarities.</li> </ol>"},{"location":"day1/module2/exercises/#exercise-23-conceptual-image-input-as-a-tool","title":"Exercise 2.3 (Conceptual): Image Input as a \"Tool\"","text":"<p>Goal: Think about how multimodal input relates to the concept of tool calling.</p> <p>Multimodal models can process different types of input, like text and images, simultaneously. This capability can be conceptually compared to having built-in tools for understanding different data modalities, as illustrated below:</p> <p></p> <ol> <li>Review the <code>tool-calling.py</code> script from Module 1, where an external <code>get_current_time</code> function was defined and called.</li> <li>Review the <code>inspect-image.py</code> script from Module 2, where an image is provided directly in the prompt using <code>image_url</code>.</li> <li>Discuss: In the context of <code>inspect-image.py</code>, could you consider the ability to process the <code>image_url</code> as a built-in \"tool\" of the multimodal model itself? Why or why not?</li> <li>How does providing the image directly differ from defining an external function like <code>describe_image_from_url(url: str)</code> that the LLM would have to explicitly call?</li> </ol>"},{"location":"day1/module2/multimodal/","title":"Day 1 - Module 2: Working with Multimodal Models","text":"<p>Objective: Understand how AI agents can perceive and reason about non-textual data like images and voice, using multimodal LLMs.</p> <p>Source Code: <code>src/02-multimodal-models/</code></p>"},{"location":"day1/module2/multimodal/#introduction","title":"Introduction","text":"<p>Humans perceive the world through multiple senses: sight, sound, touch, etc. To create truly intelligent agents, we need models that can also process information beyond just text. Multimodal models are designed to handle various input types, such as images and audio, alongside text.</p> <p>In this module, we will explore:</p> <ol> <li>Vision Capabilities: How to provide images to an LLM and ask questions about them (e.g., describing content, comparing images).</li> <li>Voice Interaction (Conceptual): Understanding the components involved in creating a voice-based agent that can listen and speak in real-time (using the provided <code>voice-agent.py</code> and <code>voice-interaction</code> app as examples).</li> </ol> <p>What is Multimodality?</p> <p>Multimodality in AI refers to the ability of models to process and understand information from multiple types of data (modalities) simultaneously, such as text, images, audio, and video. This allows for a richer understanding of context, similar to human perception.</p> <p>Note: The voice interaction examples rely on specific Azure credentials or setup beyond the basic GitHub PAT, as indicated in the <code>voice-agent.py</code> code (referencing Azure OpenAI endpoints and deployments). We will focus on the concepts and code structure.</p>"},{"location":"day1/module2/multimodal/#1-vision-inspecting-and-comparing-images","title":"1. Vision: Inspecting and Comparing Images","text":"<p>Modern LLMs like GPT-4o (and its mini version used here) can directly process images provided within the prompt.</p> <p>Helper Function (<code>get_image_data_url</code>): Both <code>inspect-image.py</code> and <code>compare-images.py</code> use a helper function (defined in the scripts, but conceptually similar to one in <code>utils.py</code> or <code>imagelibrary.py</code>) to convert local image files into base64-encoded data URLs. This format allows embedding the image directly into the API request.</p> <pre><code>import base64\n\ndef get_image_data_url(image_file: str, image_format: str) -&gt; str:\n    # ... (opens file, reads bytes, encodes to base64) ...\n    return f\"data:image/{image_format};base64,{image_data}\"\n</code></pre> <p>((Optional) Image Library:** The code imports <code>imagelibrary.VectorDatabase</code> and calls <code>database.download_images()</code>. This functionality fetches example images (<code>f1_car_url_1.jpg</code>, <code>f1_car_url_2.jpg</code>) from an online source if they are not present locally. For the workshop, ensure these images are available in the execution directory.</p>"},{"location":"day1/module2/multimodal/#inspecting-a-single-image","title":"Inspecting a Single Image","text":"<p>File: <code>src/02-multimodal-models/inspect-image.py</code></p> <p>This script sends a single image to the model and asks it to describe the content.</p> <p>Code Breakdown:</p> <ul> <li>Client Setup: Standard OpenAI client initialization.</li> <li>Prepare Image: Get the data URL for the image (<code>f1_car_url_1.jpg</code>).</li> <li>Construct the Prompt:<ul> <li>The <code>user</code> message content is now a list containing multiple parts:<ul> <li>A <code>text</code> part with the question (\"What's in this image?\").</li> <li>An <code>image_url</code> part containing the image data URL. The <code>detail</code> parameter can be used to control quality vs. cost/latency (not applicable to <code>gpt-4o-mini</code> as per current GitHub Models docs, but good practice).</li> </ul> </li> </ul> </li> </ul> <pre><code>response = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that describes images in details.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"What's in this image?\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": get_image_data_url(\"f1_car_url_1.jpg\", \"jpg\"),\n                        # \"detail\": \"low\" # Optional detail parameter\n                    },\n                },\n            ],\n        },\n    ],\n    model=model_name,\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/02-multimodal-models\n# Ensure f1_car_url_1.jpg exists (may be downloaded by imagelibrary.py)\npython inspect-image.py\n</code></pre> <p>The output will be the model's textual description of the F1 car image.</p>"},{"location":"day1/module2/multimodal/#comparing-multiple-images","title":"Comparing Multiple Images","text":"<p>File: <code>src/02-multimodal-models/compare-images.py</code></p> <p>This script sends two images to the model and asks it to compare them.</p> <p>Code Breakdown:</p> <ul> <li>Client Setup &amp; Image Prep: Similar to inspecting, but get data URLs for both images (<code>f1_car_url_1.jpg</code>, <code>f1_car_url_2.jpg</code>).</li> <li>Construct the Prompt:<ul> <li>The <code>user</code> message content list now includes the text prompt and two <code>image_url</code> parts.</li> <li>The text prompt explicitly asks the model to compare the images and list differences.</li> </ul> </li> </ul> <pre><code>response = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that describes images in details.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Look at these two pictures. Image 1 and Image 2. Are they similar List all the differences according to category, color, position and size.\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": get_image_data_url(\"f1_car_url_1.jpg\", \"jpg\"),\n                    },\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": get_image_data_url(\"f1_car_url_2.jpg\", \"jpg\"),\n                    },\n                },\n            ],\n        },\n    ],\n    model=model_name,\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/02-multimodal-models\n# Ensure f1_car_url_1.jpg and f1_car_url_2.jpg exist\npython compare-images.py\n</code></pre> <p>The output will be the model's comparison of the two F1 car images, highlighting their differences.</p>"},{"location":"day1/module2/multimodal/#2-voice-interaction-conceptual-overview","title":"2. Voice Interaction (Conceptual Overview)","text":"<p>File: <code>src/02-multimodal-models/voice-agent.py</code> *   <code>src/02-multimodal-models/voice-interaction/</code> (Flask app)</p> <p>These examples demonstrate a more complex scenario: a real-time voice conversation with an agent. This involves several components working together:</p> <ol> <li>Audio Input: Capturing audio from the user's microphone.</li> <li>Speech-to-Text (STT): Converting the user's speech into text.</li> <li>LLM Processing: Sending the text and conversation history to the LLM, which then processes it and uses tools if necessary.</li> <li>Text-to-Speech (TTS): Converting the LLM's text response back into audio.</li> <li>Audio Output: Playing the generated audio back to the user.</li> <li>Turn Detection: Determining when the user has finished speaking and when the agent should respond (Voice Activity Detection - VAD).</li> </ol> <p>Real-time Voice Pipeline</p> <p>Mic -&gt; STT -&gt; LLM (with Tools) -&gt; TTS -&gt; Speaker. Latency and turn detection (VAD) are critical for a natural conversational experience.</p> <p>Code Structure (<code>voice-agent.py</code>):</p> <ul> <li>Dependencies: Uses <code>semantic-kernel[realtime]</code>, <code>pyaudio</code>, <code>sounddevice</code>, <code>pydub</code>, indicating reliance on Semantic Kernel's real-time capabilities and audio libraries.</li> <li>Audio Handling: Uses <code>AudioPlayerWebRTC</code> and <code>AudioRecorderWebRTC</code> (from <code>utils.py</code>) for handling audio input/output streams, often via WebRTC for web applications.</li> <li>Realtime Agent: Leverages <code>OpenAIRealtimeWebRTC</code> from Semantic Kernel, which orchestrates the STT, LLM interaction, and TTS pipeline, using a specialized real-time API endpoint (indicated by the Azure endpoint/deployment variables).</li> <li>Settings: Configures <code>OpenAIRealtimeExecutionSettings</code> including:<ul> <li><code>instructions</code>: The system prompt for the agent.</li> <li><code>voice</code>: Specifies the TTS voice.</li> <li><code>turn_detection</code>: Configures server-side VAD to automatically manage conversation turns.</li> <li><code>function_choice_behavior</code>: Enables tool calling within the voice conversation.</li> </ul> </li> <li>Tools: Defines helper functions (<code>get_weather</code>, <code>get_date_time</code>, <code>goodbye</code>) using <code>@kernel_function</code> decorator, making them available as tools to the voice agent.</li> <li>Asynchronous Flow: Uses <code>asyncio</code> to manage the concurrent tasks of recording, processing, and playing audio.</li> <li>Event Loop: The main loop (<code>async for event in realtime_agent.receive(...)</code>) processes events from the real-time service, including transcribed text (<code>RealtimeTextEvent</code>) and other service events (<code>ListenEvents</code>).</li> </ul> <p>Flask App (<code>voice-interaction/app.py</code>): This is a web interface (using Flask and WebSockets) that interacts with the backend logic (using <code>voice-agent.py</code> concepts or the <code>rtmt.py</code> module mentioned in the file structure) to provide a web-based voice chat experience.</p> <p>Running the Voice Demo</p> <p>Running <code>python voice-interaction/app.py</code> requires: *   Specific Azure credentials set as environment variables (<code>AZURE_OPENAI_ENDPOINT</code>, <code>AZURE_OPENAI_API_KEY</code>, <code>AZURE_VOICE_COMPLETION_DEPLOYMENT_NAME</code>, <code>AZURE_VOICE_COMPLETION_MODEL</code>). *   Installation of additional audio dependencies (<code>pip install pyaudio sounddevice pydub semantic-kernel[realtime]</code>). *   Correct audio device configuration.</p> <p>Due to these complexities, it is best to treat this as a code walkthrough and conceptual explanation unless the workshop environment is specifically prepared for Azure voice services.</p> <p>Further Reading &amp; Resources (Multimodal Models)</p> <p>Explore these resources to learn more about multimodal LLMs:</p> <ul> <li>General Concepts &amp; Overviews:<ul> <li>Exploring Multimodal Large Language Models (GeeksforGeeks)</li> <li>Multimodal Models \u2014 LLMs That Can See and Hear (Towards Data Science)</li> <li>Multimodality and Large Multimodal Models (LMMs) (Chip Huyen Blog)</li> </ul> </li> <li>Tutorials &amp; Practical Guides:<ul> <li>Master Multimodal Data Analysis with LLMs and Python (freeCodeCamp)</li> <li>Coding a Multimodal (Vision) Language Model from scratch (YouTube)</li> <li>Multimodal Data Analysis with LLMs and Python \u2013 Tutorial (YouTube)</li> </ul> </li> <li>Academic &amp; Research Perspectives:<ul> <li>MLLM Tutorial @ CVPR 2024 (Covers architecture, instruction tuning, evaluation, and agentic MLLMs)</li> <li>Large Multimodal Models: Notes on CVPR 2023 Tutorial (arXiv Paper)</li> <li>CS25: V4 I From Large Language Models to Large Multimodal Models (Stanford Lecture Video)</li> </ul> </li> </ul> <p>This module explored how agents can interact with visual and auditory information. The next module will delve into handling complex, structured data formats like knowledge graphs and ontologies.</p>"},{"location":"day1/module2/solutions/","title":"Day 1 - Module 2: Solutions","text":"<p>These are the solutions or discussion points for the exercises in Module 2.</p>"},{"location":"day1/module2/solutions/#solution-21-describing-a-different-image","title":"Solution 2.1: Describing a Different Image","text":"<p>Assuming you saved an image as <code>my_image.jpg</code> in the <code>src/02-multimodal-models/</code> directory and wanted to ask \"What is the main animal in this image?\", you would modify <code>src/02-multimodal-models/inspect-image.py</code> like this:</p> <pre><code>import os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport base64\n\n# Assuming imagelibrary.py or similar is not strictly needed if image exists\n# from imagelibrary import VectorDatabase\n\nload_dotenv()\n\n# Helper function (include if not imported)\ndef get_image_data_url(image_file: str, image_format: str) -&gt; str:\n    if not os.path.exists(image_file):\n        # Attempt to download if using imagelibrary, otherwise raise error\n        # database = VectorDatabase()\n        # database.download_images()\n        # if not os.path.exists(image_file):\n        raise FileNotFoundError(f\"Image file not found: {image_file}\")\n\n    with open(image_file, \"rb\") as f:\n        image_data = base64.b64encode(f.read()).decode(\"utf-8\")\n    return f\"data:image/{image_format};base64,{image_data}\"\n\nclient = OpenAI(\n    base_url=\"https://models.inference.ai.azure.com\",\n    api_key=os.environ[\"GITHUB_TOKEN\"],\n)\n\nmodel_name = \"gpt-4o-mini\"\n\n# --- Modified Section --- \nimage_filename = \"my_image.jpg\" # Your image filename\nimage_format = \"jpg\" # Your image format\nuser_question = \"What is the main animal in this image?\" # Your question\n# ----------------------\n\nresponse = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that describes images in details.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": user_question, # Use the variable\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": get_image_data_url(image_filename, image_format), # Use variables\n                    },\n                },\n            ],\n        },\n    ],\n    model=model_name,\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <p>Expected Output: The model should provide a textual answer specifically addressing your question about the content of <code>my_image.jpg</code>.</p>"},{"location":"day1/module2/solutions/#solution-22-comparing-different-images-for-similarities","title":"Solution 2.2: Comparing Different Images for Similarities","text":"<p>Assuming you saved two images as <code>image_A.png</code> and <code>image_B.png</code> in the <code>src/02-multimodal-models/</code> directory, you would modify <code>src/02-multimodal-models/compare-images.py</code> like this:</p> <pre><code>import os\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport base64\n\n# Assuming imagelibrary.py or similar is not strictly needed if images exist\n# from imagelibrary import VectorDatabase\n\nload_dotenv()\n\n# Helper function (include if not imported)\ndef get_image_data_url(image_file: str, image_format: str) -&gt; str:\n    if not os.path.exists(image_file):\n        # Attempt to download if using imagelibrary, otherwise raise error\n        # database = VectorDatabase()\n        # database.download_images()\n        # if not os.path.exists(image_file):\n        raise FileNotFoundError(f\"Image file not found: {image_file}\")\n\n    with open(image_file, \"rb\") as f:\n        image_data = base64.b64encode(f.read()).decode(\"utf-8\")\n    return f\"data:image/{image_format};base64,{image_data}\"\n\nclient = OpenAI(\n    base_url=\"https://models.inference.ai.azure.com\",\n    api_key=os.environ[\"GITHUB_TOKEN\"],\n)\n\nmodel_name = \"gpt-4o-mini\"\n\n# --- Modified Section --- \nimage_1_filename = \"image_A.png\" # Your first image\nimage_1_format = \"png\"\nimage_2_filename = \"image_B.png\" # Your second image\nimage_2_format = \"png\"\nuser_question = \"Look at these two images. What features do they have in common?\" # Focus on similarities\n# ----------------------\n\nresponse = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that describes and compares images in detail.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": user_question, # Use the variable\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": get_image_data_url(image_1_filename, image_1_format), # Use variables\n                    },\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": get_image_data_url(image_2_filename, image_2_format), # Use variables\n                    },\n                },\n            ],\n        },\n    ],\n    model=model_name,\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <p>Expected Output: The model should provide a textual response focusing on the common features or characteristics shared by <code>image_A.png</code> and <code>image_B.png</code>.</p>"},{"location":"day1/module2/solutions/#solution-23-conceptual-image-input-as-a-tool","title":"Solution 2.3 (Conceptual): Image Input as a \"Tool\"","text":"<p>This is a discussion point rather than a coding solution.</p> <ul> <li> <p>Is image processing a built-in \"tool\"?</p> <ul> <li>Yes, in a conceptual sense. For multimodal models like GPT-4o, the ability to process image data provided via <code>image_url</code> is an inherent capability, much like processing text. You don't need to define an external function for it; the model handles it internally.</li> <li>It functions like a tool because it extends the model's capabilities beyond pure text, allowing it to access and reason about visual information to answer the user's query.</li> </ul> </li> <li> <p>How does it differ from an external function call?</p> <ul> <li>Integration: Image processing via <code>image_url</code> is deeply integrated into the model's architecture. An external function (<code>describe_image_from_url(url: str)</code>) requires an explicit multi-step process (LLM requests tool -&gt; Code executes tool -&gt; Code sends result back -&gt; LLM uses result), similar to the <code>get_current_time</code> example.</li> <li>Data Transfer: Providing the image via <code>image_url</code> (especially as a base64 data URL) sends the image data directly to the model endpoint in the same API call as the text prompt. An external function involves the code fetching the image from the URL, processing it (for example, with a separate vision model or library), and sending only the resulting text description back to the LLM in a subsequent API call.</li> <li>Richness of Information: The built-in capability allows the LLM to directly access the rich visual features of the image. An external tool returning only a text description loses nuances that the LLM could have perceived if it processed the image directly.</li> </ul> </li> </ul> <p>In essence, multimodal input is a more tightly integrated form of capability extension compared to the explicit request-execute-respond cycle of external tool calling.</p>"},{"location":"day1/module3/complex_data/","title":"Day 1 - Module 3: Handling Complex Data Structures","text":"<p>Objective: Learn how agents can process, generate, and utilize structured data formats like knowledge graphs and ontologies to represent and reason about complex information.</p> <p>Source Code: <code>src/03-complex-data/</code></p>"},{"location":"day1/module3/complex_data/#introduction","title":"Introduction","text":"<p>While LLMs excel at processing natural language, many real-world tasks involve structured information. Agents become more powerful when they can understand and work with data formats that explicitly define relationships and constraints. This module explores how agents can leverage:</p> <ol> <li>Knowledge Graphs: Representing entities and their relationships as nodes and edges.</li> <li>Ontologies: Formal specifications of a domain's concepts and relationships (often using standards like OWL).</li> <li>Structured Document Parsing: Extracting specific information from documents (like invoices) based on a predefined schema.</li> </ol> <p>Structured Data vs. Unstructured Data</p> <p>LLMs are inherently good at unstructured text. However, combining them with structured data representations like knowledge graphs or ontologies allows for more accurate, consistent, and verifiable reasoning, especially in complex domains.</p>"},{"location":"day1/module3/complex_data/#1-generating-knowledge-graphs","title":"1. Generating Knowledge Graphs","text":"<p>File: <code>src/03-complex-data/knowledge-graphs.py</code></p> <p>This script demonstrates using an LLM to generate a knowledge graph from a natural language query, representing the answer as structured data.</p> <p>Concept: Instead of just a text answer, we ask the model to structure its response according to a predefined Python class structure (<code>KnowledgeGraph</code>, <code>Node</code>, <code>Edge</code>) using the <code>client.beta.chat.completions.parse</code> feature (a feature allowing direct parsing into Pydantic models).</p> <p>Code Breakdown:</p> <ul> <li>Import Libraries: <code>OpenAI</code>, <code>pydantic</code> for data modeling, <code>graphviz</code> for visualization.</li> <li>Define Data Models (Pydantic):<ul> <li><code>Node</code>: Represents an entity with an ID, label, and color.</li> <li><code>Edge</code>: Represents a relationship between two nodes (source, target) with a label and color.</li> <li><code>KnowledgeGraph</code>: Contains lists of nodes and edges, and includes a <code>visualize</code> method using <code>graphviz</code> to generate an SVG image.</li> </ul> </li> </ul> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List\nimport graphviz\n\nclass Node(BaseModel):\n    id: int\n    label: str\n    color: str\n\nclass Edge(BaseModel):\n    source: int\n    target: int\n    label: str\n    color: str\n\nclass KnowledgeGraph(BaseModel):\n    nodes: List[Node] = Field(default_factory=list)\n    edges: List[Edge] = Field(default_factory=list)\n    description: str = None\n\n    def visualize(self):\n        # ... (graphviz code to generate SVG from nodes/edges) ...\n        dot.render(\"knowledge_graph\", view=False)\n</code></pre> <p>Pydantic for Structured Output</p> <p>Using Pydantic models along with features like <code>client.beta.chat.completions.parse</code> (or similar structured output mechanisms in other libraries) enables the reliable retrieval of JSON or Python objects from the LLM, matching your desired schema.</p> <ul> <li>Define Generation Function (<code>generate_graph</code>):<ul> <li>Takes user input text.</li> <li>Uses <code>client.beta.chat.completions.parse</code>:<ul> <li>Provides the input text within a prompt asking for a knowledge graph representation.</li> <li>Specifies <code>response_format = KnowledgeGraph</code>, instructing the API (or SDK) to return the response directly as a populated <code>KnowledgeGraph</code> object.</li> </ul> </li> <li>Returns the parsed <code>KnowledgeGraph</code> object.</li> </ul> </li> </ul> <pre><code>def generate_graph(input) -&gt; KnowledgeGraph:\n    completion = client.beta.chat.completions.parse(\n        model=model_name,\n        messages = [{\"role\" : \"assistant\", \"content\" : f\"\"\" Help me understand the following by describing as a detailed knowledge graph:  {input}\"\"\"}],\n        response_format = KnowledgeGraph # Key part for structured output\n    )\n    return completion.choices[0].message.parsed\n</code></pre> <ul> <li>Main Execution:<ul> <li>Calls <code>generate_graph</code> with a complex query about cars, wheels, trains, speed, and friction.</li> <li>Calls the <code>visualize()</code> method on the returned graph object to save the graph as <code>knowledge_graph.svg</code>.</li> </ul> </li> </ul> <pre><code>graph = generate_graph(\"What is the relationship between cars, wheels and trains...\")\ngraph.visualize()\n</code></pre> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/03-complex-data\n# Install pydantic and graphviz if needed: pip install pydantic graphviz\n# Ensure graphviz binaries are installed: sudo apt-get update &amp;&amp; sudo apt-get install -y graphviz\npython knowledge-graphs.py\n</code></pre> <p>After running, inspect the generated <code>knowledge_graph.svg</code> file (you will need to download it or use a browser within the environment if available) to see the visual representation of the model's understanding.</p>"},{"location":"day1/module3/complex_data/#2-creating-and-using-ontologies","title":"2. Creating and Using Ontologies","text":"<p>Ontologies provide a formal way to define the concepts, properties, and relationships within a specific domain. They allow for more rigorous and consistent reasoning.</p> <p>Ontologies vs. Knowledge Graphs</p> <p>While related, ontologies focus on defining the schema and rules of a domain (the TBox), while knowledge graphs represent the actual instances and relationships (the ABox). Ontologies provide the structure that knowledge graphs can populate.</p>"},{"location":"day1/module3/complex_data/#creating-an-ontology-from-images","title":"Creating an Ontology from Images","text":"<p>File: <code>src/03-complex-data/create_onthologies.py</code></p> <p>This script uses a multimodal model to generate an ontology in the Web Ontology Language (OWL) format based on analyzing images (screenshots of screw types in this case).</p> <p>Code Breakdown:</p> <ul> <li>Client Setup &amp; Image Prep: Standard client, uses <code>get_image_data_url</code> for multiple images (<code>screen_1.png</code> to <code>screen_4.png</code>).</li> <li>Detailed System Prompt: This is crucial. It instructs the model:<ul> <li>To act as an expert in ontology engineering.</li> <li>To generate an OWL ontology based on the provided images.</li> <li>To define classes, properties (data/object), terms, descriptions, domains, ranges, and hierarchies based on visual details and numerical values present in the images.</li> <li>To output only the OWL (XML) format.</li> </ul> </li> </ul> <pre><code>system_prompt = \"\"\"\nLook at the images below and use them as input. Generate a response based on the images.\nYou are an expert in ontology engineering. Generate an OWL ontology based on the following domain description:\nDefine classes, data properties, and object properties...\nProvide the output in OWL (XML) format and only output the ontology and nothing else\"\"\"\n</code></pre> <ul> <li>API Call: Sends the system prompt and the multiple image URLs to the model.</li> </ul> <pre><code>response = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\", # Note: Prompt is complex, put in user role content list\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": system_prompt, # The detailed instructions\n                },\n                # ... multiple image_url parts ...\n            ],\n        },\n    ],\n    model=model_name,\n)\n</code></pre> <ul> <li>Save Output: Extracts the OWL content from the response and saves it to <code>screws.xml</code>, removing potential markdown formatting.</li> </ul> <pre><code>content_owl = response.choices[0].message.content\nwith open(\"screws.xml\", \"w\") as file:\n    new_ontology = content_owl.replace(\"```xml\", \"\").replace(\"```\", \"\")\n    file.write(new_ontology)\n</code></pre> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/03-complex-data\n# Ensure screen_1.png to screen_4.png exist\npython create_onthologies.py\n</code></pre> <p>Inspect the generated <code>screws.xml</code> file. It should contain OWL definitions for different screw types based on the images.</p>"},{"location":"day1/module3/complex_data/#using-an-ontology-to-describe-an-image","title":"Using an Ontology to Describe an Image","text":"<p>File: <code>src/03-complex-data/use-onthology.py</code></p> <p>This script demonstrates how providing an existing ontology as context can help the model describe an image using the specific terminology and structure defined in that ontology.</p> <p>Code Breakdown:</p> <ul> <li>Client Setup &amp; Image Prep: Standard client, uses <code>get_image_data_url</code> for <code>reference.png</code>.</li> <li>Load Ontology: Reads the content of the previously generated <code>screws.xml</code> file.</li> <li>Construct Prompt:<ul> <li><code>system</code> message: Tells the model to use the provided ontology to describe the image.</li> <li><code>user</code> message content list:<ul> <li>A <code>text</code> part containing the entire content of the <code>screws.xml</code> ontology.</li> <li>An <code>image_url</code> part for <code>reference.png</code>.</li> </ul> </li> </ul> </li> </ul> <pre><code># Read ontology_content from screws.xml\n\nresponse = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that describes images in details. Make use of the onology provided to describe the image.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": ontology_content, # Provide the ontology as context\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": get_image_data_url(\"reference.png\", \"png\"),\n                    },\n                },\n            ],\n        },\n    ],\n    model=model_name, # Note: Uses gpt-4o, which is recommended for better ontology understanding\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/03-complex-data\n# Ensure screws.xml and reference.png exist\npython use-onthology.py\n</code></pre> <p>The output should be a description of <code>reference.png</code> that attempts to use the classes and properties defined in <code>screws.xml</code>.</p>"},{"location":"day1/module3/complex_data/#3-parsing-structured-documents-invoice-example","title":"3. Parsing Structured Documents (Invoice Example)","text":"<p>File: <code>src/03-complex-data/parse_invoice.py</code></p> <p>This script uses a multimodal model to extract information from an image of an invoice and populate a predefined XML template (XRechnung format).</p> <p>Code Breakdown:</p> <ul> <li>Download Invoice: Downloads a sample invoice image (<code>rechnungsvorlage.jpg</code>) if it doesn't exist locally.</li> <li>Client Setup &amp; Image Prep: Standard client, uses <code>get_image_data_url</code> for the downloaded <code>invoice.jpg</code>.</li> <li>Load Template &amp; Explanation: Reads the target XML structure from <code>invoice_template.xml</code> and additional context/instructions from <code>invoice_explaination.txt</code>.</li> <li>Construct System Prompt: Combines instructions (\"extract invoice information... create a XRechnung Beispiel XML...\") with the content of the template XML file. This gives the model the target schema.</li> </ul> <pre><code>system_prompt = \"\"\"\nLook at the image attached as input to extract all the invoice information... create a XRechnung Beispiel XML filled with all known values...\n\"\"\"\n# Read invoice_xml_content from invoice_template.xml\nsystem_prompt = system_prompt + invoice_xml_content\n# Optionally add content from invoice_explaination.txt\n</code></pre> <ul> <li>API Call: Sends the combined system prompt and the invoice image URL.</li> </ul> <pre><code>response = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": system_prompt,\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Extract all invoice data from this image.\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": get_image_data_url(\"invoice.jpg\", \"jpg\"),\n                    },\n                },\n            ],\n        },\n    ],\n    model=model_name,\n)\n</code></pre> <ul> <li>Save Output: Extracts the generated XML from the response and saves it to <code>invoice_parsed.xml</code>.</li> </ul> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/03-complex-data\n# Install requests if needed: pip install requests\npython parse_invoice.py\n</code></pre> <p>Inspect <code>invoice_parsed.xml</code>. Compare it with <code>invoice_template.xml</code> and the original <code>invoice.jpg</code> to see how well the model extracted the data and filled the structure.</p> <p>Further Reading &amp; Resources (Knowledge Graphs, Ontologies &amp; LLMs)</p> <p>Dive deeper into using LLMs with structured data:</p> <ul> <li>Knowledge Graph Construction &amp; Use:<ul> <li>How to Build Knowledge Graphs With LLMs (python tutorial) (YouTube)</li> <li>Constructing Knowledge Graphs From Unstructured Text Using LLMs (Neo4j Blog)</li> <li>Building Knowledge Graphs Using Python (Medium)</li> <li>Automated Knowledge Graph Construction with Large Language Models (Medium)</li> <li>llmgraph: Create knowledge graphs with LLMs (GitHub)</li> </ul> </li> <li>Integrating KGs/Ontologies with LLMs:<ul> <li>Unifying LLMs &amp; Knowledge Graphs for GenAI: Use Cases &amp; Best Practices (Neo4j Blog)</li> <li>How to Implement Knowledge Graphs and Large Language Models (LLMs) Together (Towards Data Science)</li> <li>Grounding Large Language Models with Knowledge Graphs (DataWalk)</li> <li>Using a Knowledge Graph to Implement a RAG Application (DataCamp Tutorial)</li> <li>Integrating Large Language Models and Knowledge Graphs (PDF Tutorial - Academic)</li> </ul> </li> <li>Resource Collections:<ul> <li>Knowledge Graph Tutorials and Papers (LLM Section) (GitHub)</li> </ul> </li> </ul> <p>This concludes Day 1, covering foundational interactions, multimodal inputs, and handling structured data. Day 2 will build on this by exploring how to give agents tools to solve more complex problems and implement basic autonomous agent patterns.</p>"},{"location":"day1/module3/exercises/","title":"Day 1 - Module 3: Exercises","text":"<p>These exercises are based on the concepts and code presented in Module 3: Handling Complex Data Structures (<code>src/03-complex-data/</code>).</p>"},{"location":"day1/module3/exercises/#exercise-31-generating-a-different-knowledge-graph","title":"Exercise 3.1: Generating a Different Knowledge Graph","text":"<p>Goal: Practice using the LLM to generate structured knowledge graph output for a different topic.</p> <p>Below is an example of the knowledge graph generated by the original script for the topic \"relationship between cars, wheels and trains...\". Your task is to modify the script to generate a similar graph for a different topic.</p> <p></p> <ol> <li>Open the <code>src/03-complex-data/knowledge-graphs.py</code> script.</li> <li>Locate the line where <code>generate_graph</code> is called:     <pre><code>graph = generate_graph(\"What is the relationship between cars, wheels and trains...\")\n</code></pre></li> <li>Change the input string to a different topic that involves entities and relationships, for example: \"Explain the main components of a simple web application: browser, web server, database, and how they interact.\"</li> <li>Run the script (<code>python3.11 knowledge-graphs.py</code>). Ensure <code>graphviz</code> is installed (<code>sudo apt-get update &amp;&amp; sudo apt-get install -y graphviz</code>).</li> <li>Inspect the newly generated <code>knowledge_graph.svg</code> file (you will need to download it or view it in a browser). Does the graph visually represent the components and interactions of a web application? Compare its structure to the example graph provided above.</li> </ol>"},{"location":"day1/module3/exercises/#exercise-32-using-the-ontology-with-an-irrelevant-image","title":"Exercise 3.2: Using the Ontology with an Irrelevant Image","text":"<p>Goal: Observe how providing domain-specific context (ontology) affects the model's description when the input image is not from that domain.</p> <ol> <li>Open the <code>src/03-complex-data/use-onthology.py</code> script.</li> <li>Locate the line specifying the image file:     <pre><code>\"url\": get_image_data_url(\"reference.png\", \"png\"),\n</code></pre></li> <li>Change the filename to use one of the battery images from the same folder, for example, <code>battery_1.png</code> (ensure this file exists).     <pre><code>\"url\": get_image_data_url(\"battery_1.png\", \"png\"),\n</code></pre></li> <li>Run the script (<code>python use-onthology.py</code>).</li> <li>Read the model's output. Does it try to apply terms from the <code>screws.xml</code> ontology (like screw head types, thread types) to the battery image? Does the description make sense? What does this tell you about the importance of providing relevant context to the LLM?</li> </ol>"},{"location":"day1/module3/exercises/#exercise-33-evaluating-invoice-parsing-accuracy","title":"Exercise 3.3: Evaluating Invoice Parsing Accuracy","text":"<p>Goal: Critically evaluate the performance of the LLM in extracting structured data from an image.</p> <ol> <li>Ensure you have run <code>src/03-complex-data/parse_invoice.py</code> at least once to generate <code>invoice_parsed.xml</code> and download <code>invoice.jpg</code>.</li> <li>Open the original invoice image (<code>invoice.jpg</code>) and view its contents.</li> <li>Open the extracted XML file (<code>invoice_parsed.xml</code>).</li> <li>Carefully compare the values in the XML fields against the corresponding information in the invoice image. Pay attention to:<ul> <li>Invoice Number</li> <li>Dates (Issue Date, Due Date)</li> <li>Sender/Supplier Information (Name, Address, Tax ID)</li> <li>Recipient/Customer Information</li> <li>Line Items (Description, Quantity, Unit Price, Total)</li> <li>Totals (Net Amount, Tax Amount, Gross Amount)</li> </ul> </li> <li>Make a list of any fields that were:<ul> <li>Extracted correctly.</li> <li>Extracted incorrectly (wrong value).</li> <li>Missed entirely (present in image but not in XML).</li> <li>Hallucinated (present in XML but not clearly in image).</li> </ul> </li> <li>Discuss potential reasons for any inaccuracies (e.g., image quality, unusual layout, handwriting, model limitations).</li> </ol>"},{"location":"day1/module3/solutions/","title":"Day 1 - Module 3: Solutions","text":"<p>These are the solutions or discussion points for the exercises in Module 3.</p>"},{"location":"day1/module3/solutions/#solution-31-generating-a-different-knowledge-graph","title":"Solution 3.1: Generating a Different Knowledge Graph","text":"<p>Modify <code>src/03-complex-data/knowledge-graphs.py</code>:</p> <pre><code># ... (imports, class definitions, generate_graph function remain the same) ...\n\nif __name__ == \"__main__\":\n    # --- Modified Input Query --- \n    input_text = \"Explain the main components of a simple web application: browser, web server, database, and how they interact.\"\n    # --------------------------\n\n    graph = generate_graph(input_text)\n    graph.visualize()\n    print(\"Knowledge graph generated and saved as knowledge_graph.svg\")\n</code></pre> <p>Expected Output: *   The script should run without errors (assuming <code>graphviz</code> is installed). *   A file named <code>knowledge_graph.svg</code> will be created in the <code>src/03-complex-data/</code> directory. *   Opening <code>knowledge_graph.svg</code> should display a graph with nodes representing \"Browser\", \"Web Server\", \"Database\" (and related concepts like \"HTTP Request\", \"SQL Query\", \"HTML/CSS/JS\") and edges showing the interactions (e.g., Browser sends Request to Web Server, Web Server queries Database, Database returns data to Web Server, Web Server sends Response to Browser).</p>"},{"location":"day1/module3/solutions/#solution-32-using-the-ontology-with-an-irrelevant-image","title":"Solution 3.2: Using the Ontology with an Irrelevant Image","text":"<p>Modify <code>src/03-complex-data/use-onthology.py</code>:</p> <pre><code># ... (imports, get_image_data_url, client setup remain the same) ...\n\n# Load ontology\nontology_file = \"screws.xml\"\nif not os.path.exists(ontology_file):\n    raise FileNotFoundError(f\"Ontology file not found: {ontology_file}. Run create_onthologies.py first.\")\nwith open(ontology_file, \"r\") as file:\n    ontology_content = file.read()\n\n# --- Modified Image --- \nimage_filename = \"battery_1.png\" # Changed image\nimage_format = \"png\"\n# ----------------------\n\nimage_data_url = get_image_data_url(image_filename, image_format)\n\nresponse = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that describes images in details. Make use of the onology provided to describe the image.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": ontology_content,\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": image_data_url, # Use variable\n                    },\n                },\n            ],\n        },\n    ],\n    model=\"gpt-4o\", # Using gpt-4o as in the original script\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <p>Expected Output: The model's output can vary depending on its interpretation. Possible outcomes include: *   Correctly identify the image as a battery but then awkwardly try to apply screw-related terms from the ontology (e.g., \"This battery has a cylindrical head type...\"). *   Ignore the ontology entirely and just describe the battery. *   Explicitly state that the provided ontology is not relevant to the image content.</p> <p>Discussion Point: This demonstrates that while providing context (like an ontology) can guide the LLM, the context must be relevant to the task and input. Providing irrelevant context can confuse the model or lead to incorrect/nonsensical outputs. The LLM doesn't inherently know the ontology only applies to screws; it tries to follow instructions, even if the combination doesn't logically fit.</p>"},{"location":"day1/module3/solutions/#solution-33-evaluating-invoice-parsing-accuracy","title":"Solution 3.3: Evaluating Invoice Parsing Accuracy","text":"<p>This exercise requires manual comparison and discussion. There is no single code solution.</p> <p>Steps for Evaluation:</p> <ol> <li>Open <code>invoice.jpg</code>: Use an image viewer.</li> <li>Open <code>invoice_parsed.xml</code>: Use a text editor or XML viewer.</li> <li>Compare Field by Field: Go through the XML structure and find the corresponding information on the image.</li> </ol> <p>Example Findings (Hypothetical - actual results may vary):</p> <ul> <li>Examples of Correct Extraction: Fields like Invoice Number, Issue Date, Sender Name/Address, Recipient Name/Address, and Total Amount are generally extracted correctly.</li> <li>Incorrect: A specific line item quantity is misread (e.g., '1' read as 'l'), a date format is misinterpreted, or a complex address line is split incorrectly.</li> <li>Missed: A secondary contact person or a specific payment term mentioned in small print can be missed.</li> <li>Hallucinated: The model invents a field value if it's unsure or if the template expects a field not clearly present (this is less common).</li> </ul> <p>Discussion Points: *   Image Quality: Was the <code>invoice.jpg</code> clear? OCR (Optical Character Recognition), which the multimodal model performs internally, is sensitive to image resolution, rotation, lighting, and font type. *   Layout Complexity: Was the invoice layout standard or unusual? Non-standard layouts make it harder for the model to associate labels with values. *   Template Matching: How well did the model map the visual information to the specific XML structure of the XRechnung template? Did it understand the semantic meaning of each XML tag? *   Model Limitations: Even advanced models aren't perfect. Complex tables, overlapping text, or ambiguous fields can lead to errors. *   Confidence: The model doesn't usually provide a confidence score for each extracted field, making it hard to know which values are reliable without manual verification. *   Improvement: How could accuracy be improved? (e.g., Higher resolution image, pre-processing the image, providing more detailed instructions/examples in the prompt, using a model specifically fine-tuned for document extraction, using zonal OCR if field locations are fixed).</p>"},{"location":"day2/module4/complex_problems/","title":"Day 2 - Module 4: Solving Complex Problems with Tools &amp; DSLs","text":"<p>Objective: Understand how agents can tackle complex, multi-step problems by leveraging external tools, browser automation, and structured thinking patterns like Domain Specific Languages (DSLs).</p> <p>Source Code: <code>src/04-complex-problems/</code></p>"},{"location":"day2/module4/complex_problems/#introduction","title":"Introduction","text":"<p>Day 1 focused on basic interactions and handling different data types. Day 2 moves towards more autonomous behavior. Real-world problems require agents to go beyond simple Q&amp;A or single-tool use. They need to:</p> <ul> <li>Interact with web interfaces (browsing, filling forms).</li> <li>Follow specific, domain-bound procedures.</li> <li>Break down complex goals into sequential steps.</li> <li>Conduct research across multiple sources.</li> </ul> <p>This module explores techniques shown in the <code>04-complex-problems</code> folder:</p> <ol> <li>Domain Specific Languages (DSLs): Guiding the LLM's planning and execution using a custom, restricted set of commands tailored to a specific task (e.g., logistics planning).</li> <li>Combining DSLs with Tools: Enhancing DSL-based plans by allowing the LLM to call external functions (tools) for validation or information gathering during execution.</li> <li>Process Frameworks: Using libraries like Semantic Kernel's process framework to define and manage sequential workflows.</li> <li>Browser Automation: Enabling agents to interact with websites to perform tasks like research, data extraction, or form submission.</li> </ol> <p>Moving Beyond Simple Tools</p> <p>This module marks a shift towards more sophisticated agent capabilities. By combining LLMs with structured planning (DSLs), process management, and the ability to interact with the web, we can build agents capable of handling significantly more complex tasks.</p>"},{"location":"day2/module4/complex_problems/#1-domain-specific-languages-dsls-for-structured-thinking","title":"1. Domain Specific Languages (DSLs) for Structured Thinking","text":"<p>File: <code>src/04-complex-problems/trucking-plan.py</code> *   <code>src/04-complex-problems/trucking-execute.py</code></p> <p>Instead of letting the LLM generate free-form plans, we can constrain its output to a specific set of commands (a DSL) that represent valid actions within a particular domain. This makes the agent's behavior more predictable and easier to integrate with existing systems.</p> <p>Concept (Trucking Example):</p> <p>The goal is to plan the logistics of loading boxes onto trucks and driving them.</p> <ul> <li>Define the DSL: The system prompt in <code>trucking-plan.py</code> explicitly defines a set of commands:<ul> <li><code>prepare_truck(truck_id)</code></li> <li><code>load_box_on_truck(truck_id, box_id, weight)</code></li> <li><code>calculate_weight_of_truck(truck_id)</code></li> <li><code>drive_truck_to_location(truck_id, weight, distance)</code></li> <li><code>unload_box_from_truck(truck_id, box_id, weight)</code></li> </ul> </li> <li>Provide Context: The prompt also gives rules (max weight/boxes, loading time, box weights) and examples of how user requests map to DSL commands.</li> <li>LLM Generates DSL Plan: When given a user request (e.g., \"I have a red box and 3 blue boxes... load the boxes... travel 100 km\"), the LLM's task is not to answer directly, but to generate a sequence of these DSL commands.</li> </ul> <p>Code Breakdown (<code>trucking-plan.py</code>):</p> <ul> <li><code>commandprompt</code>: Contains the DSL definition, rules, examples, and instructions for the LLM.</li> <li>API Call: Sends the <code>commandprompt</code> as the system message and the user's logistics request.</li> <li>Output: The script simply prints the LLM's response, which should be a sequence of DSL commands.</li> </ul> <pre><code># commandprompt defines the DSL and rules\nmessages=[\n    {\"role\": \"system\", \"content\": commandprompt},\n    {\"role\": \"user\", \"content\": \"I have a red box and 3 blue boxes...\"},\n]\nresponse = client.chat.completions.create(\n    messages=messages,\n    model=model_name,\n)\nprint(f\"{response.choices[0].message.content}\")\n</code></pre> <p>To Run (<code>trucking-plan.py</code>):</p> <pre><code>cd /home/ubuntu/agentic-playground/src/04-complex-problems\npython trucking-plan.py\n</code></pre> <p>Observe the output: it should be a series of <code>prepare_truck</code>, <code>load_box_on_truck</code>, etc., commands representing the plan.</p> <p>Code Breakdown (<code>trucking-execute.py</code>):</p> <p>This script takes the DSL concept further by combining it with tool calling.</p> <ul> <li>DSL Definition: The <code>commandprompt</code> is similar but now also instructs the LLM it can use tools like <code>get_current_time</code> and <code>calculate_travel_time</code>.</li> <li>Tool Definitions:<ul> <li>Python functions (<code>get_current_time</code>, <code>calculate_travel_time</code>) are defined.</li> <li>Corresponding tool schemas (<code>functions</code>) are created for the LLM.</li> </ul> </li> <li>Conversation Loop (<code>run_conversation</code>): This function manages the interaction:<ol> <li>Sends the user query, DSL prompt, and available tools to the LLM.</li> <li>Checks if the LLM response includes <code>tool_calls</code>.</li> <li>If yes: Executes the requested Python function, appends the result to the message history (with <code>role: \"tool\"</code>), and calls the LLM again with the updated history.</li> <li>If no tool call (or after a tool call response), returns the LLM's final message (which can be DSL commands or a natural language answer incorporating tool results).</li> </ol> </li> </ul> <p>To Run (<code>trucking-execute.py</code>):</p> <pre><code>cd /home/ubuntu/agentic-playground/src/04-complex-problems\n# Install pytz if needed: pip install pytz\npython trucking-execute.py\n</code></pre> <p>Observe the output. You should see the LLM calling <code>get_current_time</code> or <code>calculate_travel_time</code> and then generating the DSL plan or a final answer incorporating the calculated times.</p> <p>Benefits of DSLs</p> <ul> <li>Predictability: Constrains LLM output to valid, expected actions.</li> <li>Reliability: Reduces chances of hallucinated or nonsensical plans.</li> <li>Integration: Makes it easier to connect LLM plans to existing code or APIs that understand the DSL.</li> <li>Structured Thinking: Forces the LLM to break down problems into domain-specific steps.</li> </ul>"},{"location":"day2/module4/complex_problems/#2-process-frameworks-semantic-kernel-example","title":"2. Process Frameworks (Semantic Kernel Example)","text":"<p>File: <code>src/04-complex-problems/process-step.py</code></p> <p>For tasks that involve a clear sequence of steps, process frameworks provide a structured way to define and manage the flow.</p> <p>Concept (Semantic Kernel Process):</p> <p>This example uses Semantic Kernel's experimental process framework to create a simple two-step process: get the user's name, then display a greeting.</p> <ul> <li>Events: Define events that trigger transitions between steps (e.g., <code>NameReceived</code>, <code>ProcessComplete</code>).</li> <li>State: Define a data structure (<code>HelloWorldState</code>) to hold information shared between steps (e.g., the user's name).</li> <li>Steps: Define classes (<code>GetNameStep</code>, <code>DisplayGreetingStep</code>) inheriting from <code>KernelProcessStep</code>. Each step:<ul> <li>Can have an <code>activate</code> method for initialization.</li> <li>Contains kernel functions (<code>@kernel_function</code>) that perform the step's logic.</li> <li>Can access and modify the shared <code>state</code>.</li> <li>Emits events (<code>context.emit_event</code>) to signal completion or trigger the next step.</li> </ul> </li> <li>Process Builder: Use <code>ProcessBuilder</code> to:<ul> <li>Add the steps.</li> <li>Define the flow by linking events from one step to the activation of another (<code>process.on_input_event</code>, <code>step.on_event</code>).</li> <li>Define start and stop conditions.</li> </ul> </li> <li>Run Process: Use <code>start</code> to initiate the process with an initial event.</li> </ul> <p>Code Breakdown:</p> <ul> <li>Imports relevant Semantic Kernel process components.</li> <li>Defines <code>HelloWorldEvents</code> (Enum) and <code>HelloWorldState</code> (Pydantic model).</li> <li>Implements <code>GetNameStep</code> (gets input, updates state, emits <code>NameReceived</code>).</li> <li>Implements <code>DisplayGreetingStep</code> (receives state via event, generates greeting, prints, emits <code>ProcessComplete</code>).</li> <li><code>run_hello_world_process</code> function sets up the <code>ProcessBuilder</code>, links events to steps, builds, and starts the process.</li> </ul> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/04-complex-problems\n# Ensure semantic-kernel is installed: pip install semantic-kernel\npython process-step.py\n</code></pre> <p>The script will prompt you for your name and then print the greeting, demonstrating the sequential execution managed by the framework.</p> <p>Benefits of Process Frameworks</p> <ul> <li>Clarity: Explicitly defines the steps and transitions in a complex workflow.</li> <li>State Management: Provides a structured way to pass information between steps.</li> <li>Modularity: Encapsulates logic within individual steps.</li> <li>Extensibility: Can support more complex features like error handling, retries, and conditional branching (though not shown here).</li> </ul>"},{"location":"day2/module4/complex_problems/#3-browser-automation-for-web-interaction","title":"3. Browser Automation for Web Interaction","text":"<p>Relevant files for this section include:</p> <ul> <li><code>src/04-complex-problems/browser-use.py</code></li> <li><code>src/04-complex-problems/do-research.py</code></li> <li><code>src/04-complex-problems/apply-for-job.py</code></li> <li><code>src/04-complex-problems/find-contract.py</code></li> <li>The custom library used by these scripts: <code>src/browser_use/</code></li> </ul> <p>Many tasks require interacting with websites. Agents can use browser automation tools (like Playwright, Selenium, or the custom <code>browser_use</code> library here) to perform actions like:</p> <ul> <li>Navigating to URLs.</li> <li>Extracting text content.</li> <li>Clicking buttons or links.</li> <li>Filling input fields.</li> <li>Uploading files.</li> </ul> <p>Concept (<code>browser_use</code> Library):</p> <p>This repository includes a custom <code>browser_use</code> library (which wraps a tool like Playwright) that provides:</p> <ul> <li><code>Browser</code>: Manages the browser instance.</li> <li><code>Agent</code>: An agent class that takes a task, an LLM, a <code>Controller</code>, and a <code>Browser</code>.</li> <li><code>Controller</code>: Defines custom actions (<code>@controller.action</code>) that the agent can use. These actions often interact with the browser or local files.</li> <li><code>BrowserContext</code>: Passed to actions, allowing them to interact with the current browser page.</li> <li><code>ActionResult</code>: A standard return type for actions, including extracted content or error messages.</li> </ul> <p>The <code>Agent</code> works in a loop: 1.  Perceive: Get the current page state (HTML, visible elements, URL). 2.  Plan: Send the page state, task description, and available actions (from the <code>Controller</code>) to the LLM. 3.  Act: The LLM decides the next action (e.g., <code>click</code>, <code>input</code>, <code>scroll</code>, or a custom action like <code>read_cv</code> or <code>save_jobs</code>). The <code>Agent</code> executes the action via the <code>Browser</code> or <code>Controller</code>. 4.  Repeat until the task is complete or an error occurs.</p> <p>The Perception-Planning-Action Loop</p> <p>This iterative cycle is fundamental to many autonomous agents. The agent perceives its environment (the webpage), plans its next move based on its goal and capabilities (available actions), and then executes that action, changing the environment and starting the loop again.</p> <p>Code Examples:</p> <ul> <li><code>browser-use.py</code>: A very simple example tasking the agent to compare prices by browsing the web.</li> <li><code>do-research.py</code>:<ul> <li>Task: Research battery chemistry based on a taxonomy file and specific websites.</li> <li>Custom Actions: <code>read_taxonomy</code>, <code>save_insights</code> (saves findings to JSON).</li> <li>Demonstrates reading local files (<code>battery_chem.xml</code>) and saving structured output.</li> </ul> </li> <li><code>apply-for-job.py</code>:<ul> <li>Task: Find job offers, evaluate against a CV, apply.</li> <li>Custom Actions: <code>read_cv</code> (reads PDF), <code>save_jobs</code> (saves to CSV), <code>upload_cv</code> (interacts with file input elements on a webpage).</li> <li>Shows PDF reading and file upload interaction.</li> </ul> </li> <li><code>find-contract.py</code>:<ul> <li>Task: Find electricity contracts based on usage data.</li> <li>Custom Actions: <code>read_file</code> (reads usage data/location), <code>save_results</code> (saves contracts to CSV).</li> <li>Illustrates using predefined user data within the browsing task.</li> </ul> </li> </ul> <p>To Run (Example: <code>do-research.py</code>):</p> <pre><code>cd /home/ubuntu/agentic-playground/src/04-complex-problems\n# Ensure dependencies are installed (requires browser_use setup, playwright, langchain, openai, pydantic)\n# Requires specific browser setup (e.g., Chrome path in BrowserConfig)\n# Ensure battery_chem.xml exists\npython do-research.py\n</code></pre> <p>Running Browser Automation</p> <p>Running these examples can be complex due to the custom <code>browser_use</code> library and specific browser setup needs (installing browsers, drivers, handling paths). Focus on understanding the pattern: LLM decides -&gt; Action executed -&gt; Page state updated -&gt; LLM decides again.</p> <p>Further Reading &amp; Resources</p> <p>Explore these topics further:</p> <ul> <li>Domain Specific Languages (DSLs) &amp; LLMs:<ul> <li>Large Language Models for Domain-Specific Language Generation (Medium - Part 1)</li> <li>Large Language Models for Domain-Specific Language Generation (Medium - Part 2: Constraints)</li> <li>Building Domain-Specific LLMs: Examples and Techniques (Kili Technology)</li> <li>What is a Domain-Specific LLM? Examples and Benefits (Aisera)</li> </ul> </li> <li>Browser Automation &amp; Web Agents:<ul> <li>Playwright Documentation (Popular browser automation library)</li> <li>Selenium Documentation (Another popular browser automation library)</li> <li>LangChain Browser Integration (Example of integrating browsing tools with agents)</li> <li>Building Autonomous Agents that Browse the Web (YouTube - AssemblyAI)</li> </ul> </li> <li>Process Frameworks / Orchestration:<ul> <li>Semantic Kernel Documentation (Explore planners and orchestration features)</li> <li>LangChain Agents: Introduction (Covers different agent types and execution loops)</li> </ul> </li> </ul> <p>This module demonstrated how agents can tackle more complex problems using structured approaches like DSLs, process frameworks, and browser automation. The next module will focus on implementing single, autonomous agents using patterns like ReAct.</p>"},{"location":"day2/module4/exercises/","title":"Day 2 - Module 4: Exercises","text":"<p>These exercises are based on the concepts and code presented in Module 4: Solving Complex Problems with Tools &amp; DSLs (<code>src/04-complex-problems/</code>).</p>"},{"location":"day2/module4/exercises/#exercise-41-modifying-the-trucking-dsl-plan","title":"Exercise 4.1: Modifying the Trucking DSL Plan","text":"<p>Goal: Observe how changing the input query affects the DSL plan generated by the LLM.</p> <ol> <li>Open the <code>src/04-complex-problems/trucking-plan.py</code> script.</li> <li>Locate the <code>user</code> message content:     <pre><code>{\"role\": \"user\", \"content\": \"I have a red box and 3 blue boxes...\"},\n</code></pre></li> <li>Modify the <code>content</code> to describe a different scenario. For example:<ul> <li>\"I have 2 green boxes (10kg each) and 1 yellow box (5kg). Load them onto truck 1 and travel 250 km.\"</li> <li>\"Load 5 small packages (2kg each) onto truck 2. Calculate the total weight and then drive 50 km.\"</li> </ul> </li> <li>Run the script (<code>python trucking-plan.py</code>).</li> <li>Analyze the output. Does the generated sequence of DSL commands (<code>prepare_truck</code>, <code>load_box_on_truck</code>, etc.) accurately reflect the new scenario you described?</li> </ol>"},{"location":"day2/module4/exercises/#exercise-42-triggering-a-tool-in-the-trucking-dsl-execution","title":"Exercise 4.2: Triggering a Tool in the Trucking DSL Execution","text":"<p>Goal: Modify the input to encourage the LLM to use an available tool within the DSL execution flow.</p> <ol> <li>Open the <code>src/04-complex-problems/trucking-execute.py</code> script.</li> <li>Locate the <code>user_query</code> variable:     <pre><code>user_query = \"I have a red box and 3 blue boxes...\"\n</code></pre></li> <li>Modify the <code>user_query</code> to explicitly ask for information that requires a tool. For example:<ul> <li>\"Load the red box (15kg) and blue box (8kg) onto truck 1. If the current time is before noon, travel 100 km. Otherwise, travel 50 km.\"</li> <li>\"Prepare truck 3. Load a 20kg package. Calculate the travel time for a 300 km journey and tell me the estimated arrival time based on the current time.\"</li> </ul> </li> <li>Run the script (<code>python trucking-execute.py</code>). Ensure <code>pytz</code> is installed (<code>pip install pytz</code>).</li> <li>Observe the verbose output (if enabled in the script, or by adding <code>verbose=True</code> to <code>run_conversation</code> if needed). Does the LLM call the <code>get_current_time</code> or <code>calculate_travel_time</code> tool? Does the final output (DSL plan or natural language answer) incorporate the result from the tool?</li> </ol>"},{"location":"day2/module4/exercises/#exercise-43-conceptual-designing-a-tea-making-dsl","title":"Exercise 4.3 (Conceptual): Designing a Tea-Making DSL","text":"<p>Goal: Practice defining a Domain Specific Language for a simple, everyday process.</p> <p>A Domain Specific Language (DSL) is a set of commands tailored for a specific task, allowing an agent (or human) to express complex instructions concisely. The trucking example uses a DSL for logistics. The diagram below shows how a DSL fits into the agent's process:</p> <p></p> <ol> <li>Imagine you want an agent to make a cup of tea. Define a set of simple commands (a DSL) that represent the necessary actions.    *   Think about the parameters each command needs.<ul> <li>Examples: <code>boil_water(kettle_id)</code>, <code>get_cup(cup_id)</code>, <code>add_tea_bag(cup_id, tea_type)</code>, <code>pour_water(kettle_id, cup_id)</code>, <code>add_milk(cup_id, amount_ml)</code>, <code>add_sugar(cup_id, spoons)</code>, <code>wait(seconds)</code>, <code>serve(cup_id)</code>.</li> </ul> </li> <li>Write down your list of DSL commands and their parameters.</li> <li>Write a sample plan using your DSL commands to make a cup of black tea with milk and two sugars, letting it steep for 60 seconds.</li> </ol>"},{"location":"day2/module4/exercises/#exercise-44-conceptual-adding-summarization-to-research-agent","title":"Exercise 4.4 (Conceptual): Adding Summarization to Research Agent","text":"<p>Goal: Think about how to modify an existing agent task and add new capabilities (tools).</p> <ol> <li>Consider the <code>do-research.py</code> example (even if you couldn't run it due to dependencies). Its task is to research battery chemistry based on a taxonomy file and save insights to JSON using the <code>save_insights</code> action.</li> <li>Suppose you want the agent to summarize the key findings in a short paragraph in addition to saving the detailed insights.</li> <li>How could you modify the agent's overall <code>task</code> description given to the <code>Agent</code> class to include this summarization requirement?</li> <li>Would you need a new custom action (e.g., <code>@controller.action(\"summarize_findings\")</code>)? If so, what would that action need to do? Could the summarization be handled by the LLM itself as part of its final response without a dedicated tool?</li> </ol>"},{"location":"day2/module4/solutions/","title":"Day 2 - Module 4: Solutions","text":"<p>These are the solutions or discussion points for the exercises in Module 4.</p>"},{"location":"day2/module4/solutions/#solution-41-modifying-the-trucking-dsl-plan","title":"Solution 4.1: Modifying the Trucking DSL Plan","text":"<p>Modify <code>src/04-complex-problems/trucking-plan.py</code>:</p> <pre><code># ... (imports, client setup, commandprompt definition remain the same) ...\n\n# --- Modified User Query --- \nuser_input = \"I have 2 green boxes (10kg each) and 1 yellow box (5kg). Load them onto truck 1 and travel 250 km.\"\n# -------------------------\n\nmessages=[\n    {\"role\": \"system\", \"content\": commandprompt},\n    {\"role\": \"user\", \"content\": user_input},\n]\n\nresponse = client.chat.completions.create(\n    messages=messages,\n    model=model_name,\n)\n\nprint(f\"{response.choices[0].message.content}\")\n</code></pre> <p>Expected Output: The output should be a sequence of DSL commands similar to this (exact box IDs can vary):</p> <p><pre><code>prepare_truck(truck_id=1)\nload_box_on_truck(truck_id=1, box_id=1, weight=10)\nload_box_on_truck(truck_id=1, box_id=2, weight=10)\nload_box_on_truck(truck_id=1, box_id=3, weight=5)\ncalculate_weight_of_truck(truck_id=1)\ndrive_truck_to_location(truck_id=1, weight=25, distance=250)\n</code></pre> This demonstrates the LLM correctly interpreting the new box quantities, weights, truck ID, and distance, and translating them into the defined DSL.</p>"},{"location":"day2/module4/solutions/#solution-42-triggering-a-tool-in-the-trucking-dsl-execution","title":"Solution 4.2: Triggering a Tool in the Trucking DSL Execution","text":"<p>Modify <code>src/04-complex-problems/trucking-execute.py</code>:</p> <pre><code># ... (imports, tool definitions, client setup, commandprompt, run_conversation remain the same) ...\n\n# --- Modified User Query --- \nuser_query = \"Prepare truck 3. Load a 20kg package. Calculate the travel time for a 300 km journey and tell me the estimated arrival time based on the current time.\"\n# -------------------------\n\nresponse = run_conversation(user_query)\nprint(response)\n</code></pre> <p>Expected Output: *   The script execution trace should show calls to <code>get_current_time</code> (without arguments, or the LLM invents a location if not specified) and <code>calculate_travel_time</code> (with <code>distance=300</code>). *   The final output from the <code>print(response)</code> statement will be a natural language sentence incorporating the results, such as: \"Okay, truck 3 is prepared with the 20kg package. The current time is [time from tool]. The estimated travel time for 300 km is [time from tool]. Therefore, the estimated arrival time is approximately [calculated arrival time].\" *   Alternatively, the LLM can also output some DSL commands mixed with the tool calls and final answer, depending on its interpretation.</p> <p>This shows the LLM using the available tools (<code>get_current_time</code>, <code>calculate_travel_time</code>) when the query explicitly requires information provided by those tools, integrating the results into its response.</p>"},{"location":"day2/module4/solutions/#solution-43-conceptual-designing-a-tea-making-dsl","title":"Solution 4.3 (Conceptual): Designing a Tea-Making DSL","text":"<p>This requires defining commands and a sample plan.</p> <p>Example DSL Commands:</p> <ul> <li><code>boil_water(kettle_id: str)</code>: Starts boiling water in the specified kettle.</li> <li><code>check_kettle_boiled(kettle_id: str) -&gt; bool</code>: Checks if the water in the kettle has boiled.</li> <li><code>get_cup(cup_id: str)</code>: Retrieves a clean cup.</li> <li><code>add_tea_bag(cup_id: str, tea_type: str)</code>: Places a tea bag of the specified type into the cup.</li> <li><code>pour_water(kettle_id: str, cup_id: str)</code>: Pours boiled water from the kettle into the cup.</li> <li><code>wait(seconds: int)</code>: Pauses execution for the specified duration (for steeping).</li> <li><code>remove_tea_bag(cup_id: str)</code>: Removes the tea bag from the cup.</li> <li><code>add_milk(cup_id: str, amount_ml: int)</code>: Adds the specified amount of milk.</li> <li><code>add_sugar(cup_id: str, spoons: int)</code>: Adds the specified number of spoons of sugar.</li> <li><code>stir(cup_id: str)</code>: Stirs the contents of the cup.</li> <li><code>serve(cup_id: str)</code>: Presents the finished cup of tea.</li> </ul> <p>Sample Plan (Black tea, milk, 2 sugars, 60s steep):</p> <pre><code>get_cup(cup_id=\"my_cup\")\nadd_tea_bag(cup_id=\"my_cup\", tea_type=\"black_tea\")\nboil_water(kettle_id=\"kitchen_kettle\")\n# Loop or wait until check_kettle_boiled(kettle_id=\"kitchen_kettle\") returns true\npour_water(kettle_id=\"kitchen_kettle\", cup_id=\"my_cup\")\nwait(seconds=60)\nremove_tea_bag(cup_id=\"my_cup\")\nadd_milk(cup_id=\"my_cup\", amount_ml=30)\nadd_sugar(cup_id=\"my_cup\", spoons=2)\nstir(cup_id=\"my_cup\")\nserve(cup_id=\"my_cup\")\n</code></pre>"},{"location":"day2/module4/solutions/#solution-44-conceptual-adding-summarization-to-research-agent","title":"Solution 4.4 (Conceptual): Adding Summarization to Research Agent","text":"<p>This requires discussing modifications to the agent's task or tools.</p> <p>1. Modifying the Task Description:</p> <p>You could modify the <code>task</code> string given to the <code>Agent</code> class in <code>do-research.py</code>:</p> <ul> <li>Original (Conceptual): \"Research battery chemistry based on <code>battery_chem.xml</code> and specific websites. Save detailed insights using <code>save_insights</code> action.\"</li> <li>Modified: \"Research battery chemistry based on <code>battery_chem.xml</code> and specific websites. Save detailed insights using <code>save_insights</code> action. After saving the insights, provide a concise summary paragraph of the key findings.\"</li> </ul> <p>Adding the summarization requirement directly to the task description is sufficient if the underlying LLM is capable enough. It performs the research, calls <code>save_insights</code>, and then generates the summary as its final output.</p> <p>2. Adding a New Custom Action:</p> <p>If simply modifying the task isn't reliable, or if you want more control over the summarization process (e.g., summarizing specific parts of the saved data), you could add a new action:</p> <pre><code># In the Controller definition within do-research.py or browser_use library\n\n@controller.action(\n    \"summarize_findings\",\n    params={\"findings_text\": \"Text containing the research findings to be summarized\"}\n)\ndef summarize_findings(findings_text: str):\n    \"\"\"Generates a concise summary paragraph of the provided text.\"\"\"\n    # Option 1: Call the LLM again specifically for summarization\n    summary_prompt = f\"Summarize the following research findings into a single concise paragraph:\\n\\n{findings_text}\"\n    # Assume 'llm' is accessible here or passed in\n    summary = llm.invoke(summary_prompt) # Or use the agent's LLM instance\n    logger.info(f\"Generated summary: {summary}\")\n    # This action returns the summary text, which the agent \n    # includes in its final response, or it logs it.\n    return ActionResult(extracted_content=summary, include_in_memory=True) \n\n    # Option 2: Use a dedicated summarization library (less common needed here)\n</code></pre> <p>Integration: *   The LLM, guided by the modified task description, would need to decide to call <code>summarize_findings</code> after gathering and saving the research data. *   It would need to retrieve the gathered text (from the <code>ActionResult</code> of previous steps or by reading the saved JSON file via another tool) to pass as input to <code>summarize_findings</code>.</p> <p>Which approach is better? *   Modifying the task description is simpler if the LLM can handle the sequential requirement (research -&gt; save -&gt; summarize). *   Adding a dedicated tool gives more explicit control and is necessary if the summarization needs specific logic or needs to operate on structured data retrieved from the saved file.</p>"},{"location":"day2/module5/exercises/","title":"Day 2 - Module 5: Exercises","text":"<p>These exercises are based on the concepts and code presented in Module 5: Implementing Single Agents (ReAct Pattern) (<code>src/05-single-agent/</code>). Choose one of the framework examples (<code>react-agent-lc.py</code>, <code>react-agent-li.py</code>, or <code>reasoning-agent-sk.py</code>) to work with for exercises 5.1 and 5.2.</p>"},{"location":"day2/module5/exercises/#exercise-51-triggering-multiple-tools","title":"Exercise 5.1: Triggering Multiple Tools","text":"<p>Goal: Modify the input query to require the agent to use a sequence of tools to find the answer.</p> <ol> <li>Choose one of the agent scripts (<code>react-agent-lc.py</code>, <code>react-agent-li.py</code>, or <code>reasoning-agent-sk.py</code>).</li> <li>Modify the initial user input/query. Instead of asking for the time, ask a question that requires chaining information from multiple tools. For example:<ul> <li>\"What is the weather like for Dennis right now?\" (Requires <code>get_current_username</code> -&gt; <code>get_current_location_of_user</code> -&gt; <code>get_weather</code>)</li> <li>\"What time is it where Dennis is located?\" (Requires <code>get_current_username</code> -&gt; <code>get_current_location_of_user</code> -&gt; <code>get_current_time</code>)</li> </ul> </li> <li>Run the chosen script.</li> <li>Observe the verbose output (enable <code>verbose=True</code> if needed in LangChain/LlamaIndex, or observe the sequence of calls in Semantic Kernel). Does the agent correctly identify the need for multiple tools? Does it call them in a logical order (e.g., get username first, then location, then weather/time)? Does it arrive at the correct final answer?</li> </ol>"},{"location":"day2/module5/exercises/#exercise-52-adding-a-new-tool","title":"Exercise 5.2: Adding a New Tool","text":"<p>Goal: Practice extending the agent's capabilities by adding a new tool.</p> <ol> <li>Choose one of the agent scripts.</li> <li>Define a new tool function: Add a simple Python function, for example, a basic calculator.     <pre><code>def add_numbers(a: int, b: int) -&gt; int:\n    \\\"\\\"\\\"Adds two integer numbers together.\\\"\\\"\\\"\n    print(f\"Tool: Adding {a} + {b}\")\n    return a + b\n</code></pre></li> <li>Integrate the tool:<ul> <li>LangChain (<code>react-agent-lc.py</code>): Add the <code>@tool</code> decorator above your <code>add_numbers</code> function and include the function object in the <code>tools</code> list passed to <code>create_react_agent</code>.</li> <li>LlamaIndex (<code>react-agent-li.py</code>): Create a <code>FunctionTool</code> from your function (<code>add_tool = FunctionTool.from_defaults(fn=add_numbers)</code>) and add <code>add_tool</code> to the <code>tools</code> list passed to <code>ReActAgent.from_tools</code>.</li> <li>Semantic Kernel (<code>reasoning-agent-sk.py</code>): Add the <code>add_numbers</code> function to the <code>ChefPlugin</code> class in <code>plugins.py</code> and decorate it with <code>@kernel_function</code>.</li> </ul> </li> <li>Modify the input query: Change the user input to ask a question that requires the new tool, e.g., \"What is 25 plus 17?\".</li> <li>Run the chosen script.</li> <li>Observe the output. Does the agent recognize the need for the <code>add_numbers</code> tool? Does it call the tool with the correct arguments (25 and 17)? Does it provide the correct sum (42) in its final answer?</li> </ol>"},{"location":"day2/module5/exercises/#exercise-53-conceptual-handling-tool-errors-in-react","title":"Exercise 5.3 (Conceptual): Handling Tool Errors in ReAct","text":"<p>Goal: Think about how an agent reacts when a tool fails.</p> <ol> <li>Consider the ReAct loop: Thought -&gt; Action -&gt; Action Input -&gt; Observation -&gt; Thought...</li> <li>Imagine the <code>get_weather</code> tool fails (e.g., the API is down, invalid location provided) and returns an error message or raises an exception.</li> <li>What should the agent do in its next \"Thought\" step after receiving this error \"Observation\"?<ul> <li>Should it try the same tool again?</li> <li>Should it try a different tool?</li> <li>Should it inform the user it cannot get the weather?</li> <li>Should it try to guess the weather?</li> </ul> </li> <li>How does the specific prompt given to the ReAct agent (like the ones in <code>react-agent-lc.py</code> or <code>react-agent-li.py</code>) influence how it handles errors? Do the prompts explicitly mention error handling?</li> <li>How does the <code>handle_parsing_errors=True</code> option in LangChain's <code>AgentExecutor</code> relate to this? Does it handle tool execution errors or just errors in parsing the LLM's output?</li> </ol>"},{"location":"day2/module5/single_agent/","title":"Day 2 - Module 5: Implementing Single Agents (ReAct Pattern)","text":"<p>Objective: Understand and implement the ReAct (Reason + Act) pattern to create autonomous agents that can use tools iteratively to solve problems.</p> <p>Source Code: <code>src/05-single-agent/</code></p>"},{"location":"day2/module5/single_agent/#introduction","title":"Introduction","text":"<p>Having explored how to equip LLMs with tools and structure their thinking (Module 4), we now focus on building agents that can autonomously decide which tools to use and when to use them to accomplish a given task. A popular pattern for this is ReAct (Reason + Act).</p> <p>The ReAct pattern involves an iterative loop:</p> <ol> <li>Thought: The agent reasons about the current state, the overall goal, and what action is needed next.</li> <li>Action: The agent decides to use a specific tool.</li> <li>Action Input: The agent determines the necessary input for the chosen tool.</li> <li>Observation: The agent receives the result (output) from executing the tool.</li> <li>The agent incorporates the observation into its reasoning (back to step 1) and continues the loop until the final answer is determined.</li> </ol> <p>This module demonstrates the ReAct pattern using three different frameworks:</p> <ol> <li>LangChain: Using <code>create_react_agent</code> and <code>AgentExecutor</code>.</li> <li>LlamaIndex: Using the <code>ReActAgent</code> class.</li> <li>Semantic Kernel: Using <code>ChatCompletionAgent</code> with auto function calling (implicitly follows a similar reasoning loop).</li> </ol> <p>All examples use a similar set of tools defined in <code>plugins.py</code> (for Semantic Kernel) or directly in the scripts (for LangChain/LlamaIndex) for getting user info, location, and time.</p>"},{"location":"day2/module5/single_agent/#shared-tools-concept","title":"Shared Tools Concept","text":"<ul> <li><code>plugins.py</code> (for Semantic Kernel): Defines a <code>ChefPlugin</code> class with methods decorated by <code>@kernel_function</code>. These methods (<code>get_weather</code>, <code>get_medical_history</code>, <code>get_available_incredients</code>, <code>get_current_username</code>, <code>get_current_location_of_user</code>, <code>get_current_time</code>) become tools available to the Semantic Kernel agent.</li> <li>LangChain/LlamaIndex Examples: Define similar functions (<code>get_current_username</code>, <code>get_current_location</code>, <code>get_current_time</code>) and wrap them using framework-specific decorators or classes (<code>@tool</code> for LangChain, <code>FunctionTool.from_defaults</code> for LlamaIndex) to make them usable by the respective agents.</li> </ul>"},{"location":"day2/module5/single_agent/#1-react-with-langchain","title":"1. ReAct with LangChain","text":"<p>File: <code>src/05-single-agent/react-agent-lc.py</code></p> <p>This script uses LangChain to create a ReAct agent.</p> <p>Code Breakdown:</p> <ul> <li>Import Libraries: <code>langchain</code>, <code>langchain_openai</code>, etc.</li> <li>Initialize LLM: Sets up the <code>ChatOpenAI</code> client.</li> <li>Define Tools: Uses the <code>@tool</code> decorator on Python functions (<code>get_current_username</code>, <code>get_current_location</code>, <code>get_current_time</code>) to make them LangChain tools.</li> <li>Define ReAct Prompt Template:<ul> <li>A specific prompt (<code>promptString</code>) instructs the LLM on the ReAct process:<ul> <li>Explains the <code>Thought</code>, <code>Action</code>, <code>Action Input</code>, <code>Observation</code> sequence.</li> <li>Lists available tools (<code>{tools}</code>, <code>{tool_names}</code>).</li> <li>Specifies the format for the final answer.</li> <li>Includes placeholders for the user input (<code>{input}</code>) and the agent's internal scratchpad (<code>{agent_scratchpad}</code>).</li> </ul> </li> <li><code>PromptTemplate.from_template(promptString)</code> creates the template object.</li> </ul> </li> <li>Create ReAct Agent: <code>create_react_agent(llm, tools, prompt)</code> combines the LLM, tools, and the specific ReAct prompt.</li> <li>Create Agent Executor: <code>agents.AgentExecutor(...)</code> wraps the agent logic. It handles the execution loop:<ul> <li>Calls the agent to get the next action.</li> <li>Executes the action (tool).</li> <li>Passes the observation back to the agent.</li> <li>Repeats until the agent provides a final answer.</li> <li><code>verbose=True</code> shows the Thought/Action/Observation steps.</li> <li><code>handle_parsing_errors=True</code> attempts to recover if the LLM output doesn't perfectly match the expected format.</li> <li><code>max_iterations</code> prevents infinite loops.</li> </ul> </li> <li>Invoke Executor: <code>agent_executor.invoke({\"input\": input})</code> starts the process with the user's question.</li> </ul> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/05-single-agent\n# Ensure langchain, langchain-openai, pytz are installed\npython react-agent-lc.py\n</code></pre> <p>Observe the verbose output. You will see the agent's reasoning steps (Thought), the tools it chooses (Action), the inputs it provides (Action Input), and the results it gets back (Observation) as it works towards answering \"What is the current time here?\". It should first get the username, then the location, then the time for that location.</p>"},{"location":"day2/module5/single_agent/#2-react-with-llamaindex","title":"2. ReAct with LlamaIndex","text":"<p>File: <code>src/05-single-agent/react-agent-li.py</code></p> <p>This script implements the same ReAct logic using LlamaIndex.</p> <p>Code Breakdown:</p> <ul> <li>Import Libraries: <code>llama_index.core.agent</code>, <code>llama_index.llms.openai</code>, <code>llama_index.core.tools</code>, etc.</li> <li>Define Tools: Uses <code>FunctionTool.from_defaults(fn=...)</code> to wrap the Python functions (<code>get_current_username</code>, <code>get_current_location</code>, <code>get_current_time</code>) into LlamaIndex tools.</li> <li>Initialize LLM: Uses <code>OpenAILike</code> to connect to the GitHub Models endpoint (or standard <code>OpenAI</code> if configured differently).</li> <li>Define ReAct System Prompt: Similar to LangChain, a detailed system prompt (<code>react_system_header_str</code>) explains the Thought/Action/Action Input/Observation format and rules to the LLM. <code>PromptTemplate(react_system_header_str)</code> creates the template.</li> <li>Create ReAct Agent: <code>ReActAgent.from_tools(...)</code> creates the agent:<ul> <li>Takes the list of tools.</li> <li>Takes the LLM instance.</li> <li>Can optionally take a custom <code>react_chat_formatter</code> and system prompt (the example relies on the default prompt structure expected by <code>ReActAgent</code>; the commented <code>agent.update_prompts</code> shows customization is possible).</li> <li><code>verbose=True</code> enables detailed logging of the ReAct steps.</li> </ul> </li> <li>Chat with Agent: <code>agent.stream_chat(...)</code> initiates the conversation. LlamaIndex's ReAct agent handles the iterative tool use internally based on the prompt structure.</li> <li>Stream Response: The example uses <code>response_gen.print_response_stream()</code> to print the agent's thoughts and final answer as they are generated.</li> </ul> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/05-single-agent\n# Ensure llama-index, llama-index-llms-openai, pytz are installed\npython react-agent-li.py\n</code></pre> <p>Again, observe the output stream. You should see the agent reasoning, selecting tools (<code>get_current_username</code>, <code>get_current_location</code>, <code>get_current_time</code>), executing them, and finally providing the answer.</p>"},{"location":"day2/module5/single_agent/#3-reasoning-agent-with-semantic-kernel","title":"3. Reasoning Agent with Semantic Kernel","text":"<p>File: <code>src/05-single-agent/reasoning-agent-sk.py</code></p> <p>This script uses Semantic Kernel's agent capabilities. While not explicitly called \"ReAct\" in the code, the <code>ChatCompletionAgent</code> with automatic function calling enabled achieves a similar iterative reasoning and tool-use loop.</p> <p>Code Breakdown:</p> <ul> <li>Import Libraries: <code>semantic_kernel</code>, <code>semantic_kernel.agents</code>, <code>semantic_kernel.connectors.ai.open_ai</code>, etc.</li> <li>Import Plugin: Imports <code>ChefPlugin</code> from <code>plugins.py</code>.</li> <li>Create Kernel: A helper function <code>_create_kernel_with_chat_completion</code> sets up a Semantic Kernel instance, adds the <code>OpenAIChatCompletion</code> service, and crucially, adds the <code>ChefPlugin</code>.</li> <li>Configure Function Calling:<ul> <li><code>_create_chat_completion_client</code> gets the kernel and execution settings.</li> <li><code>execution_settings.function_choice_behavior = FunctionChoiceBehavior.Auto()</code> tells the kernel/LLM to automatically decide when to call functions (tools) from the added plugin based on the conversation.</li> </ul> </li> <li>Create Chat Completion Agent: <code>ChatCompletionAgent(...)</code> creates the agent instance, providing the kernel (with the plugin and function calling enabled) and instructions.</li> <li>Chat Loop:<ul> <li>The <code>main</code> function runs an interactive loop.</li> <li><code>agent_expert.invoke(messages=user_input, thread=thread)</code> sends the user message to the agent.</li> <li>Semantic Kernel handles the interaction with the LLM. If the LLM decides a function call is needed (based on the prompt, history, and available functions in <code>ChefPlugin</code>), the kernel executes it and sends the result back to the LLM to continue processing, mimicking the ReAct cycle.</li> <li>The loop prints messages from the agent (which could be intermediate thoughts/tool calls if configured, or the final response).</li> </ul> </li> </ul> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/05-single-agent\n# Ensure semantic-kernel, pytz are installed\npython reasoning-agent-sk.py\n</code></pre> <p>Enter the question \"What is the current time here?\" at the <code>User:&gt;</code> prompt. Observe the output. While it does not explicitly show \"Thought/Action/Observation\" like the other frameworks by default, the agent will call <code>get_current_username</code>, <code>get_current_location_of_user</code>, and <code>get_current_time</code> from the <code>ChefPlugin</code> before giving the final answer, demonstrating the underlying reasoning and tool use.</p> <p>This module introduced the ReAct pattern for building single autonomous agents using LangChain, LlamaIndex, and Semantic Kernel. The next module will explore how to introduce human oversight and intervention into agent workflows.</p>"},{"location":"day2/module5/solutions/","title":"Day 2 - Module 5: Solutions","text":"<p>These are the solutions or discussion points for the exercises in Module 5.</p>"},{"location":"day2/module5/solutions/#solution-51-triggering-multiple-tools","title":"Solution 5.1: Triggering Multiple Tools","text":"<p>Goal: Observe the agent calling multiple tools in sequence.</p> <p>Modification (Example using LangChain - <code>react-agent-lc.py</code>):</p> <p><pre><code># ... (imports, tool definitions, llm setup, prompt template remain the same) ...\n\nif __name__ == \"__main__\":\n    # --- Modified Input Query --- \n    input = \"What is the weather like for Dennis right now?\"\n    # --------------------------\n\n    tools = [get_current_username, get_current_location, get_current_time, get_weather] # Ensure get_weather is defined and included\n    prompt = PromptTemplate.from_template(promptString)\n    agent = create_react_agent(llm, tools, prompt)\n    agent_executor = agents.AgentExecutor(\n        agent=agent, \n        tools=tools, \n        verbose=True, \n        handle_parsing_errors=True, \n        max_iterations=5 # Increase max_iterations if needed\n    )\n\n    response = agent_executor.invoke({\"input\": input})\n    print(response)\n</code></pre> (Note: You would need to define a <code>get_weather(location: str)</code> tool similar to the others for this specific query)</p> <p>Expected Output (Conceptual - Verbose Log):</p> <pre><code>&gt; Entering new AgentExecutor chain...\nThought: I need to find the weather for Dennis. First, I need to know who Dennis is, but the tools suggest I can get the current username directly. Let me get the username.\nAction: get_current_username\nAction Input: {}\nObservation: Dennis\nThought: Okay, the user is Dennis. Now I need Dennis's location to get the weather.\nAction: get_current_location\nAction Input: {\"username\": \"Dennis\"} \nObservation: Berlin\nThought: Dennis is in Berlin. Now I can get the weather for Berlin.\nAction: get_weather\nAction Input: {\"location\": \"Berlin\"}\nObservation: The weather in Berlin is currently sunny with a temperature of 22\u00b0C.\nThought: I have the weather information for Dennis in Berlin. I can now provide the final answer.\nFinal Answer: The weather for Dennis in Berlin is currently sunny with a temperature of 22\u00b0C.\n\n&gt; Finished chain.\n`The agent calls `get_current_username`, then `get_current_location`, then `get_weather` before giving the final answer.Solution 5.2: Adding a New Tool\n\n**Goal:** Extend the agent with a calculator tool.\n\n**Modification (Example using LlamaIndex - [`react-agent-li.py`](https://github.com/denniszielke/agentic-playground/blob/main/src/05-single-agent/react-agent-li.py){target=\"_blank\"}):**\n\n```python\n# ... (imports, other tool definitions, llm setup remain the same) ...\n\n# --- Define New Tool Function --- \ndef add_numbers(a: int, b: int) -&gt; int:\n    \"\"\"Adds two integer numbers together.\"\"\"\n    print(f\"Tool: Adding {a} + {b}\")\n    return a + b\n# -------------------------------\n\n# --- Create FunctionTool for the new function --- \nadd_tool = FunctionTool.from_defaults(fn=add_numbers)\n# ------------------------------------------------\n\n# --- Include the new tool in the list --- \ntools = [\n    FunctionTool.from_defaults(fn=get_current_username),\n    FunctionTool.from_defaults(fn=get_current_location),\n    FunctionTool.from_defaults(fn=get_current_time),\n    add_tool, # Add the new tool here\n]\n# ----------------------------------------\n\n# ... (LLM setup, agent creation remain the same, ensure tools list is passed) ...\n\nif __name__ == \"__main__\":\n    # ... (agent setup) ...\n    # --- Modified Input Query --- \n    response_gen = agent.stream_chat(\"What is 25 plus 17?\")\n    # --------------------------\n    response_gen.print_response_stream()\n</code></pre> <p>Expected Output (Conceptual - Stream):</p> <p>The output stream should show the agent reasoning: *   Thought: The user wants to add 25 and 17. I have a tool called <code>add_numbers</code> that can do this. *   Action: <code>add_numbers</code> *   Action Input: <code>{\"a\": 25, \"b\": 17}</code> *   (Console output will show: <code>Tool: Adding 25 + 17</code>) *   Observation: <code>42</code> *   Thought: The result of adding 25 and 17 is 42. I can now answer the user. *   Final Answer: 25 plus 17 is 42.</p>"},{"location":"day2/module5/solutions/#solution-53-conceptual-handling-tool-errors-in-react","title":"Solution 5.3 (Conceptual): Handling Tool Errors in ReAct","text":"<p>This is a discussion point.</p> <ol> <li> <p>Agent's Reaction to Error: When a tool returns an error observation (e.g., \"Error: Weather API unavailable\" or \"Error: Invalid location provided\"), the agent's next \"Thought\" step should ideally:</p> <ul> <li>Acknowledge the error: Recognize that the previous action failed.</li> <li>Analyze the error: Understand why it failed, if possible (e.g., was it the tool itself, or the input provided?).</li> <li>Re-plan: Decide on the next best course of action. This could be:<ul> <li>Retry (Cautiously): If the error is transient (like a temporary API glitch), it can try the same tool again.</li> <li>Try Alternative: If the error suggests bad input (e.g., invalid location), it can try to get the input again (e.g., ask the user for clarification or use <code>get_current_location</code> again).</li> <li>Inform User: If the tool is fundamentally broken or the required information cannot be obtained, the agent should inform the user about the failure (e.g., \"I tried to get the weather, but the weather service is currently unavailable.\").</li> <li>Abandon Goal (Rare): If the failed tool was critical and there's no alternative, it may have to report it cannot complete the task.</li> </ul> </li> <li>Avoid Guessing: The agent should generally avoid making up information if a tool fails.</li> </ul> </li> <li> <p>Influence of the Prompt: The ReAct system prompt is crucial. If the prompt includes instructions on how to handle errors or failed actions (e.g., \"If a tool returns an error, report the error to the user and stop\" or \"If a tool fails, try to find an alternative way to get the information\"), the LLM will follow that guidance. The prompts in the examples do not explicitly detail error handling, relying on the LLM's general reasoning.</p> </li> <li> <p><code>handle_parsing_errors</code> in LangChain: This option specifically deals with situations where the LLM's output (its Thought/Action/Action Input block) doesn't conform to the format the <code>AgentExecutor</code> expects. For example, if the LLM forgets to specify an action or formats the JSON input incorrectly. <code>handle_parsing_errors=True</code> allows the executor to send the malformed output back to the LLM as an \"Observation\" with instructions to correct the format. It does not directly handle errors that occur during the execution of the tool itself (like an API failure). Tool execution errors are typically caught by the code calling the tool, and the error message is returned as the \"Observation\" for the agent to reason about in the next step.</p> </li> </ol>"},{"location":"day2/module6/exercises/","title":"Day 2 - Module 6: Exercises","text":"<p>These exercises are based on the concepts and code presented in Module 6: Human-in-the-Loop Patterns (<code>src/06-human-in-the-loop/</code>).</p>"},{"location":"day2/module6/exercises/#exercise-61-changing-the-interrupt-message","title":"Exercise 6.1: Changing the Interrupt Message","text":"<p>Goal: Practice modifying the information presented to the user when a workflow is interrupted.</p> <ol> <li>Open the <code>src/06-human-in-the-loop/interrupt.py</code> script.</li> <li>Locate the <code>node2</code> function.</li> <li>Find the line where <code>interrupt()</code> is called:     <pre><code>human_input = await interrupt({\"ai_answer\": \"What is your age?\"})\n</code></pre></li> <li>Change the dictionary passed to <code>interrupt</code>. For example, change the question or add more context:     <pre><code>human_input = await interrupt({\"prompt_for_user\": \"Please provide your favorite color to continue.\", \"current_step\": \"node2\"})\n</code></pre></li> <li>Run the script (<code>python interrupt.py</code>).</li> <li>Observe the output when the script pauses. Does it display the new message or data structure you provided in the <code>interrupt</code> call?</li> </ol>"},{"location":"day2/module6/exercises/#exercise-62-modifying-the-reviewer-agent","title":"Exercise 6.2: Modifying the Reviewer Agent","text":"<p>Goal: Observe how changing an agent's instructions affects its behavior in a collaborative loop.</p> <ol> <li>Open the <code>src/06-human-in-the-loop/report-agents.py</code> script.</li> <li>Part A: Change Initial Input:<ul> <li>Locate the <code>main</code> function and the initial user prompt:     <pre><code>user_input = input(\"User: \")\n</code></pre></li> <li>Run the script (<code>python report-agents.py</code>). When prompted for <code>User:</code>, provide a different piece of text than the example, e.g., \"Write a tweet about the importance of testing software.\"</li> <li>Observe the interaction between the Writer and Reviewer agents.</li> </ul> </li> <li>Part B: Modify Reviewer Instructions:<ul> <li>Stop the script if it's running.</li> <li>Find the <code>REVIEWER_INSTRUCTIONS</code> variable.</li> <li>Modify the instructions to make the reviewer stricter or more lenient. For example:<ul> <li>Stricter: Add a requirement like \"Ensure the text is less than 100 characters and includes a hashtag.\"</li> <li>Lenient: Change the instructions to something like \"Briefly check for major grammatical errors only. Approve if it looks reasonable.\"</li> </ul> </li> <li>Run the script again, providing the same initial input as in Part A.</li> <li>Compare the interaction to Part A. Does the reviewer's feedback change? Does the conversation take more or fewer turns to complete?</li> </ul> </li> </ol>"},{"location":"day2/module6/exercises/#exercise-63-conceptual-modifying-interrupt-for-approval","title":"Exercise 6.3 (Conceptual): Modifying Interrupt for Approval","text":"<p>Goal: Think about how to adapt the interrupt pattern for a yes/no approval step.</p> <ol> <li>Consider the <code>interrupt.py</code> example where <code>node2</code> interrupts to get the user's age.</li> <li>Imagine you want <code>node2</code> to present some data (e.g., <code>result = \"Plan A\"</code>) and ask the human for approval before <code>node3</code> runs.</li> <li>How would you modify <code>node2</code>?<ul> <li>What dictionary would you pass to <code>interrupt()</code>? (e.g., <code>{\"data_to_approve\": result, \"prompt\": \"Do you approve this plan? (yes/no)\"}</code>)</li> <li>How would you check the <code>human_input</code> received after the interrupt?</li> </ul> </li> <li>How would you modify the graph structure (<code>builder.add_edge</code>)? Conditional edges would be needed after <code>node2</code>.<ul> <li>If <code>human_input</code> is \"yes\", add an edge from <code>node2</code> to <code>node3</code>.</li> <li>If <code>human_input</code> is \"no\", what should happen? Add an edge back to <code>node1</code>? Add an edge to a new \"handle_rejection\" node? Add an edge directly to <code>END</code>?</li> </ul> </li> </ol>"},{"location":"day2/module6/human_loop/","title":"Day 2 - Module 6: Human-in-the-Loop Patterns","text":"<p>Objective: Understand why and how to incorporate human oversight, feedback, and intervention into agent workflows.</p> <p>Source Code: <code>src/06-human-in-the-loop/</code></p>"},{"location":"day2/module6/human_loop/#introduction","title":"Introduction","text":"<p>While a common goal is to create fully autonomous agents, there are many scenarios where human involvement is crucial or desirable:</p> <ul> <li>Safety and Control: Preventing agents from taking harmful or unintended actions, especially in high-stakes environments.</li> <li>Quality Assurance: Reviewing or correcting agent outputs before they are finalized or used in downstream processes.</li> <li>Guidance and Decision Making: Providing input when the agent faces ambiguity, requires subjective judgment, or needs to choose between multiple valid options.</li> <li>Learning and Improvement: Using human feedback to fine-tune agent behavior, improve prompts, or identify areas for model retraining.</li> </ul> <p>This module explores patterns for integrating humans into the agent execution loop, focusing on:</p> <ol> <li>Interruptible Workflows (LangGraph): Pausing agent execution at specific points to wait for human input before proceeding.</li> <li>Collaborative Review Loops (Semantic Kernel): Designing multi-agent systems where one agent's output is reviewed (by a human or another agent acting as a proxy) before further action.</li> </ol> <p>Why Human-in-the-Loop (HIL)?</p> <p>HIL combines the strengths of AI (speed, scale, data processing) with human capabilities (judgment, common sense, ethical reasoning, handling ambiguity). It's essential for building trustworthy, reliable, and effective AI systems in many real-world applications.</p>"},{"location":"day2/module6/human_loop/#1-interruptible-workflows-with-langgraph","title":"1. Interruptible Workflows with LangGraph","text":"<p>File: <code>src/06-human-in-the-loop/interrupt.py</code></p> <p>LangGraph is a library (often used with LangChain) for building stateful, multi-actor applications, including agentic workflows, as graphs. It explicitly supports interrupting the graph execution to wait for external input.</p> <p>Concept:</p> <ul> <li>State Graph: The workflow is defined as a graph where nodes represent processing steps (functions) and edges represent transitions based on the current state.</li> <li>State: A shared dictionary (<code>State</code> TypedDict) holds data passed between nodes.</li> <li>Nodes: Asynchronous functions that take the current state and return updates to the state.</li> <li>Interrupt: A special function <code>interrupt()</code> can be called within a node. This pauses the graph execution and signals that human input is needed.</li> <li>Checkpointer: Required for interrupts. It saves the graph's state when interrupted, allowing it to be resumed later (<code>MemorySaver</code> used here for in-memory checkpointing).</li> <li>Resuming: The graph is resumed by sending a special <code>Command(resume=...)</code> object containing the human input.</li> </ul> <p>Code Breakdown:</p> <ul> <li>Import Libraries: <code>langgraph</code>, <code>asyncio</code>, <code>uuid</code>.</li> <li>Define State: <code>State</code> TypedDict with <code>input</code>, <code>ai_answer</code>, <code>human_answer</code> fields.</li> <li>Define Nodes: <code>node1</code>, <code>node2</code>, <code>node3</code> are simple async functions simulating work.<ul> <li><code>node2</code> calls <code>interrupt({\"ai_answer\": \"What is your age?\"})</code>. This pauses the graph and makes the dictionary available to the client waiting for the interrupt.</li> <li>The value returned by <code>interrupt()</code> is the input provided when resuming.</li> </ul> </li> <li>Build Graph:<ul> <li><code>StateGraph(State)</code> initializes the graph builder.</li> <li><code>builder.add_node(...)</code> adds the nodes.</li> <li><code>builder.add_edge(...)</code> defines the fixed sequence: START -&gt; node1 -&gt; node2 -&gt; node3 -&gt; END.</li> </ul> </li> <li>Compile Graph: <code>builder.compile(checkpointer=MemorySaver())</code> creates the runnable graph, enabling checkpointing.</li> <li>Run Graph (<code>run_graph</code> function):<ol> <li>Creates a unique <code>thread_id</code> for the conversation state.</li> <li>Starts the graph execution using <code>graph.astream(...)</code> with an initial input.</li> <li>Iterates through the output chunks until the graph interrupts (implicitly, when <code>node2</code> calls <code>interrupt</code>).</li> <li>Prompts the user for input (<code>resume=input(...)</code>).</li> <li>Creates a <code>Command(resume=...)</code> object with the user's input.</li> <li>Resumes the graph execution by sending the <code>Command</code> via <code>graph.astream(command, config)</code>.</li> <li>Continues iterating through chunks until the graph reaches the END.</li> </ol> </li> </ul> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/06-human-in-the-loop\n# Ensure langgraph is installed: pip install langgraph\npython interrupt.py\n</code></pre> <p>Observe the output. The program will print the progress through <code>node1</code>, then pause after <code>node2</code> prints its message and asks for the interrupt command (your age). Enter an age. The graph will resume, <code>node2</code> will receive the age, <code>node3</code> will execute, and the graph will finish.</p> <p>Key LangGraph Concepts for HIL</p> <ul> <li>Graphs: Define the workflow structure.</li> <li><code>interrupt()</code>: The core function to pause for human input.</li> <li>Checkpointer: Essential for saving and resuming state across interruptions.</li> <li><code>Command(resume=...)</code>: The mechanism to inject human input and continue the flow.</li> </ul>"},{"location":"day2/module6/human_loop/#2-collaborative-review-loops-semantic-kernel-agentgroupchat","title":"2. Collaborative Review Loops (Semantic Kernel AgentGroupChat)","text":"<p>File: <code>src/06-human-in-the-loop/report-agents.py</code></p> <p>This example uses Semantic Kernel's <code>AgentGroupChat</code> to simulate a scenario where a \"Writer\" agent drafts content, and a \"Reviewer\" agent provides feedback. The loop continues until the Reviewer deems the content satisfactory. While the Reviewer is an AI agent here, it simulates the role a human can play in a review process.</p> <p>Concept (Semantic Kernel AgentGroupChat):</p> <ul> <li>Agents: Two <code>ChatCompletionAgent</code> instances are created: <code>agent_writer</code> and <code>agent_reviewer</code>, each with specific instructions defining their roles.</li> <li>Group Chat: An <code>AgentGroupChat</code> manages the interaction between the agents (and the user, if involved).</li> <li>Selection Strategy: Determines which agent speaks next. Here, <code>KernelFunctionSelectionStrategy</code> uses an LLM prompt (<code>selection_function</code>) that enforces a turn order (User -&gt; Writer -&gt; Reviewer -&gt; Writer -&gt; ...).</li> <li>Termination Strategy: Determines when the chat should end. Here, <code>KernelFunctionTerminationStrategy</code> uses another LLM prompt (<code>termination_function</code>) to check if the Reviewer's last message indicates satisfaction (contains \"yes\").</li> <li>Chat Loop: The <code>main</code> function takes user input (the initial content or feedback), adds it to the chat, and then calls <code>chat.invoke()</code>. The <code>AgentGroupChat</code> orchestrates the conversation based on the selection and termination strategies:<ul> <li>Writer revises content based on user input/reviewer feedback.</li> <li>Reviewer checks the writer's output.</li> <li>If satisfactory, the reviewer indicates approval, triggering termination.</li> <li>If not satisfactory, the reviewer provides feedback, and the loop continues with the writer.</li> </ul> </li> </ul> <p>Code Breakdown:</p> <ul> <li>Agent Definitions: <code>ChatCompletionAgent</code> instances for <code>agent_reviewer</code> and <code>agent_writer</code> with detailed instructions.</li> <li>Selection Prompt (<code>selection_function</code>): Defines rules for turn-taking based on the last speaker.</li> <li>Termination Prompt (<code>termination_function</code>): Defines rules for ending the chat based on the reviewer's response indicating satisfaction.</li> <li><code>AgentGroupChat</code> Setup: Configures the chat with the agents and the selection/termination strategies.</li> <li>Main Loop: Takes user input, adds it to the chat, invokes the chat, and prints agent responses until <code>chat.is_complete</code> is true (due to termination strategy or max iterations).</li> </ul> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/06-human-in-the-loop\n# Ensure semantic-kernel, pyperclip are installed\npython report-agents.py\n</code></pre> <p>Enter some initial text at the <code>User:&gt;</code> prompt (e.g., \"Draft a short paragraph about the benefits of AI agents.\"). Observe the interaction: the Writer will draft, the Reviewer will critique, the Writer will revise, and so on, until the Reviewer is satisfied (or the iteration limit is reached). You can provide feedback as the user during the loop.</p> <p>Simulating Human Review</p> <p>This pattern demonstrates how agent interactions can model human review processes. A human could replace the <code>agent_reviewer</code>, or the <code>AgentGroupChat</code> could be integrated with a UI that presents the Writer's output to a human for explicit approval or feedback before the next step. This is a powerful way to ensure quality and alignment with human expectations.</p> <p>Further Reading &amp; Resources (Human-in-the-Loop)</p> <p>Explore how different frameworks implement HIL:</p> <ul> <li>LangGraph Documentation: Human-in-the-loop</li> <li>LlamaIndex Documentation: Human in the loop</li> <li>Tutorials &amp; Examples:<ul> <li>Build Your First Human-in-the-Loop AI Agent with NVIDIA NIM (NVIDIA Blog)</li> <li>Hands-on Tutorial: Building an AI Agent with human-in-the-loop control (SAP Community)</li> <li>n8n Tutorial: AI Agents with Human Feedback (YouTube)</li> <li>How to Build AI Agents with Human-In-The-Loop (YouTube)</li> </ul> </li> <li>Conceptual Discussions:<ul> <li>Agents with Human in the Loop : Everything You Need to Know (Dev.to)</li> </ul> </li> </ul> <p>This concludes Day 2, covering how agents can use tools, DSLs, process frameworks, browser automation, implement autonomous patterns like ReAct, and incorporate human interaction points. Day 3 will focus on more complex architectures involving multiple agents collaborating.</p>"},{"location":"day2/module6/solutions/","title":"Day 2 - Module 6: Solutions","text":"<p>These are the solutions or discussion points for the exercises in Module 6.</p>"},{"location":"day2/module6/solutions/#solution-61-changing-the-interrupt-message","title":"Solution 6.1: Changing the Interrupt Message","text":"<p>Modify <code>src/06-human-in-the-loop/interrupt.py</code>:</p> <pre><code># ... (imports, State definition, node1, node3 remain the same) ...\n\nasync def node2(state: State):\n    print(\"---Node 2--- AI is asking for input\")\n    # --- Modified Interrupt Call --- \n    human_input = await interrupt({\"prompt_for_user\": \"Please provide your favorite color to continue.\", \"current_step\": \"node2\"})\n    # ---------------------------\n    print(f\"---Node 2--- Got human input: {human_input}\")\n    return {\"human_answer\": human_input}\n\n# ... (graph building and run_graph function remain the same) ...\n</code></pre> <p>Expected Output: When the script pauses after <code>node1</code>, instead of asking for age, the output before the input prompt should reflect the new dictionary passed to <code>interrupt</code>. The exact display depends on how the <code>astream</code> consumer prints the interrupt information, but it should contain the keys <code>prompt_for_user</code> and <code>current_step</code> with their corresponding values.</p>"},{"location":"day2/module6/solutions/#solution-62-modifying-the-reviewer-agent","title":"Solution 6.2: Modifying the Reviewer Agent","text":"<p>Part A: Change Initial Input: Running the script and providing \"Write a tweet about the importance of testing software.\" at the <code>User:</code> prompt will initiate the Writer/Reviewer loop for this new topic.</p> <p>Part B: Modify Reviewer Instructions: Modify <code>src/06-human-in-the-loop/report-agents.py</code>:</p> <pre><code># ... (imports, other agent definitions remain the same) ...\n\n# --- Modified Reviewer Instructions --- \nREVIEWER_INSTRUCTIONS = \"\"\"\nYou are a strict reviewer. \nReview the text provided by the writer. \nEnsure the text is less than 100 characters and includes at least one hashtag. \nProvide detailed feedback if it does not meet the criteria. \nIf it meets the criteria, respond with only the word \"yes\".\n\"\"\"\n# ------------------------------------\n\n# ... (rest of the script, including agent creation and chat loop, remains the same) ...\n</code></pre> <p>Expected Output Comparison (Part A vs. Part B): *   Part A (Original Reviewer): The reviewer provides general feedback on clarity or style. The number of turns is relatively small if the initial draft is decent. *   Part B (Stricter Reviewer): The reviewer will specifically check for length (&lt; 100 chars) and the presence of a hashtag. If the writer's initial draft (or revisions) fails these checks, the reviewer will provide targeted feedback (e.g., \"The text is too long.\", \"Missing hashtag.\"). This will lead to more revision cycles (more turns in the conversation) before the reviewer responds with \"yes\".</p>"},{"location":"day2/module6/solutions/#solution-63-conceptual-modifying-interrupt-for-approval","title":"Solution 6.3 (Conceptual): Modifying Interrupt for Approval","text":"<p>This requires discussing changes to the node and graph structure.</p> <p>1. Modifying <code>node2</code>:</p> <pre><code>async def node2(state: State):\n    print(\"---Node 2--- Presenting plan for approval\")\n    plan_result = \"Plan A: Use algorithm X\" # Simulate some result\n\n    # Interrupt to ask for approval\n    human_input = await interrupt({\n        \"data_to_approve\": plan_result, \n        \"prompt\": f\"Proposed plan: \n{plan_result}\n\nDo you approve? (yes/no)\"\n    })\n\n    print(f\"---Node 2--- Got human input: {human_input}\")\n    # Store the decision in the state for conditional branching\n    return {\"human_approval\": human_input.lower().strip()}\n</code></pre> <p>2. Modifying Graph Structure:</p> <p>You need to define a function to handle the conditional logic and use <code>builder.add_conditional_edges</code>.</p> <p><pre><code>from langgraph.graph import StateGraph, END\nfrom typing import Literal\n\n# ... (State definition including 'human_approval': str) ...\n# ... (node1, node2, node3 definitions) ...\n\n# Define a new node to handle rejection (optional)\nasync def handle_rejection(state: State):\n    print(\"---Node Rejection--- Plan was not approved.\")\n    # Can loop back or end here\n    return {}\n\n# Conditional edge logic\ndef check_approval(state: State) -&gt; Literal[\"approved\", \"rejected\"]:\n    if state.get(\"human_approval\") == \"yes\":\n        return \"approved\"\n    else:\n        return \"rejected\"\n\n# Build the graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"node1\", node1)\nbuilder.add_node(\"node2\", node2) # Node 2 now returns 'human_approval'\nbuilder.add_node(\"node3\", node3) # The 'approved' path\nbuilder.add_node(\"rejection_node\", handle_rejection) # The 'rejected' path\n\nbuilder.set_entry_point(\"node1\")\nbuilder.add_edge(\"node1\", \"node2\")\n\n# Add conditional edges after node2\nbuilder.add_conditional_edges(\n    \"node2\",\n    check_approval, # Function to determine the branch\n    {\n        \"approved\": \"node3\", # If check_approval returns \"approved\", go to node3\n        \"rejected\": \"rejection_node\" # If check_approval returns \"rejected\", go to rejection_node\n    }\n)\n\n# Define endpoints\nbuilder.add_edge(\"node3\", END)\nbuilder.add_edge(\"rejection_node\", END) # Or loop back, e.g., builder.add_edge(\"rejection_node\", \"node1\")\n\n# Compile the graph (with checkpointer)\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\n# ... (run_graph function would work similarly, handling the interrupt from node2) ...\n</code></pre> This structure allows the graph to branch based on the human's \"yes\" or \"no\" input received during the interrupt.</p>"},{"location":"day3/module7/exercises/","title":"Day 3 - Module 7: Exercises","text":"<p>These exercises are based on the concepts and code presented in Module 7: Multi-Agent Collaboration (<code>src/07-multi-agent-collaboration/</code>).</p>"},{"location":"day3/module7/exercises/#exercise-71-changing-the-coding-task-multi-agent","title":"Exercise 7.1: Changing the Coding Task (Multi-Agent)","text":"<p>Goal: Observe how the Coder/Reviewer LangGraph agents handle a different programming task.</p> <ol> <li>Open the <code>src/07-multi-agent-collaboration/coding-agents.py</code> script.</li> <li>Locate the <code>query</code> variable near the end of the script:     <pre><code>query = \"Create a function in python that trains a regression model...\"\n</code></pre></li> <li>Change the <code>query</code> to a different, relatively simple coding task. Examples:<ul> <li>\"Write a Python function that takes a list of numbers and returns the sum of squares.\"</li> <li>\"Create a Python class <code>Rectangle</code> with methods to calculate area and perimeter.\"</li> <li>\"Write a Python function to check if a given string is a palindrome.\"</li> </ul> </li> <li>Run the script (<code>python coding-agents.py</code>).</li> <li>Observe the output. Does the Coder agent generate appropriate code for the new task? Does the Reviewer provide relevant feedback? Does the process converge to a reasonable solution?</li> </ol>"},{"location":"day3/module7/exercises/#exercise-72-adjusting-review-criteria-multi-agent","title":"Exercise 7.2: Adjusting Review Criteria (Multi-Agent)","text":"<p>Goal: Modify the reviewer agent's instructions to influence the collaboration process.</p> <ol> <li>Open the <code>src/07-multi-agent-collaboration/coding-agents.py</code> script.</li> <li>Locate the <code>reviewer_start</code> prompt string variable.</li> <li>Modify the prompt to add a specific requirement for the reviewer. For example, add:<ul> <li>\"Ensure that all functions include type hints.\"</li> <li>\"Verify that the code includes comments explaining complex logic.\"</li> <li>\"Check if the code handles potential edge cases or errors (e.g., empty lists, invalid input).\"</li> </ul> </li> <li>Run the script using the original regression model <code>query</code> (or the query from Exercise 7.1).</li> <li>Compare the reviewer's feedback to previous runs. Does the reviewer now specifically mention the new criterion you added? Does this change how the Coder revises the code?</li> </ol>"},{"location":"day3/module7/exercises/#exercise-73-comparing-single-vs-multi-agent-output","title":"Exercise 7.3: Comparing Single vs. Multi-Agent Output","text":"<p>Goal: Compare the code generated by the single reasoning agent (<code>reasoning-coder.py</code>) with the final code produced by the multi-agent system (<code>coding-agents.py</code>) for the same task.</p> <ol> <li>Choose a coding task (e.g., the original regression model task, or the factorial/palindrome task from Exercise 7.1).</li> <li>Run <code>coding-agents.py</code> with your chosen task and note the final code generated after the review cycles.</li> <li>Run <code>reasoning-coder.py</code> with the exact same task description.</li> <li>Compare the code generated by both scripts:<ul> <li>Does the code achieve the objective in both cases?</li> <li>Are there differences in code style, structure, or quality?</li> <li>Did the multi-agent review process catch issues or lead to improvements that the single agent missed?</li> <li>Which approach seemed more effective or efficient for this specific task?</li> </ul> </li> </ol>"},{"location":"day3/module7/exercises/#exercise-74-conceptual-designing-a-trip-planning-graph","title":"Exercise 7.4 (Conceptual): Designing a Trip Planning Graph","text":"<p>Goal: Practice designing a multi-agent workflow using the LangGraph conceptual model.</p> <ol> <li>Imagine creating a multi-agent system to help a user plan a trip.</li> <li>Define potential agent roles. Examples:<ul> <li><code>FlightFinder</code>: Searches for flights based on origin, destination, dates.</li> <li><code>HotelFinder</code>: Searches for hotels based on location, dates, budget.</li> <li><code>ActivityPlanner</code>: Suggests activities based on interests and location.</li> <li><code>ItineraryCompiler</code>: Gathers information from other agents and creates a structured itinerary.</li> <li><code>UserInteraction</code>: Handles communication with the user (getting preferences, presenting options, asking for confirmation).</li> </ul> </li> <li>Sketch a diagram or list the steps for a possible workflow using these agents in a LangGraph structure.<ul> <li>What would the <code>GraphState</code> need to hold (e.g., destination, dates, flight_options, hotel_options, chosen_flight, chosen_hotel, itinerary)?</li> <li>Where would conditional edges be needed (e.g., based on user choices)?</li> <li>Where would human-in-the-loop interrupts be needed (e.g., for the user to select a flight or hotel)?</li> </ul> </li> </ol>"},{"location":"day3/module7/multi_agent/","title":"Day 3 - Module 7: Multi-Agent Collaboration","text":"<p>Objective: Explore patterns and frameworks for building systems where multiple specialized agents collaborate to achieve a common goal.</p> <p>Source Code: <code>src/07-multi-agent-collaboration/</code></p>"},{"location":"day3/module7/multi_agent/#introduction","title":"Introduction","text":"<p>While single agents (Module 5) can handle complex tasks, breaking down problems and assigning specialized roles to different agents leads to more robust, maintainable, and scalable solutions. Multi-agent systems allow for:</p> <ul> <li>Specialization: Each agent focuses on a specific task or expertise (e.g., coding, reviewing, planning, research).</li> <li>Modularity: Easier to develop, test, and update individual agents.</li> <li>Complex Problem Decomposition: Tackling problems too large or diverse for a single agent.</li> </ul> <p>This module explores different approaches to multi-agent collaboration, contrasting a structured workflow using LangGraph with a more reasoning-focused approach.</p> <ol> <li>Structured Collaboration (LangGraph): Defining explicit roles and interaction patterns (e.g., a Coder agent and a Reviewer agent improving code iteratively).</li> <li>Reasoning-Based Approach (AutoGen/MagenticOne): Using a powerful reasoning model (like o1-mini) to handle tasks that would otherwise require multiple specialized agents.</li> </ol> <p>The Power of Collaboration</p> <p>Just like human teams, multi-agent systems leverage diverse skills and parallel processing to solve problems more effectively than a single entity can. Designing the communication and workflow between agents is key.</p>"},{"location":"day3/module7/multi_agent/#1-structured-collaboration-coder-reviewer-langgraph","title":"1. Structured Collaboration: Coder &amp; Reviewer (LangGraph)","text":"<p>File: <code>src/07-multi-agent-collaboration/coding-agents.py</code></p> <p>This script implements a multi-agent system using LangGraph where two agents, a Coder and a Reviewer, collaborate to generate and refine code based on an initial objective.</p> <p>Concept:</p> <ul> <li>Roles:<ul> <li>Coder: Writes or improves code based on an objective or feedback.</li> <li>Reviewer: Reviews the code provided by the Coder, checking for PEP8 compliance, bugs, and adherence to the objective, providing feedback.</li> </ul> </li> <li>Workflow (Graph): The interaction follows a defined cycle:<ol> <li>Initial code is generated (or provided).</li> <li>Reviewer checks the code and provides feedback.</li> <li>A decision node checks if the feedback indicates the code is satisfactory or if the iteration limit is reached.</li> <li>If not satisfactory, the Coder receives the feedback and revises the code.</li> <li>The revised code goes back to the Reviewer (Step 2).</li> <li>If satisfactory, the process ends, followed by a final rating.</li> </ol> </li> <li>LangGraph Implementation:<ul> <li><code>StateGraph(GraphState)</code> defines the structure.</li> <li><code>GraphState</code> holds shared information (objective, code, feedback, history, iterations, etc.).</li> <li>Nodes (<code>handle_reviewer</code>, <code>handle_coder</code>, <code>handle_result</code>) encapsulate the logic for each agent/step, interacting with an LLM.</li> <li>Conditional Edges (<code>workflow.add_conditional_edges</code>) implement the decision logic based on the reviewer's feedback (using an LLM call <code>classify_feedback</code> to interpret if the feedback is addressed).</li> </ul> </li> </ul> <p>Code Breakdown:</p> <ul> <li>Import Libraries: <code>langgraph</code>, <code>langchain_openai</code>, <code>pydantic</code>, etc.</li> <li>Initialize LLM: <code>ChatOpenAI</code> client.</li> <li>Define State (<code>GraphState</code>): TypedDict holding all relevant information passed between nodes.</li> <li>Define Nodes:<ul> <li><code>handle_reviewer</code>: Takes current code and specialization, calls LLM with reviewer instructions, returns feedback and increments iteration count.</li> <li><code>handle_coder</code>: Takes current code, feedback, and specialization, calls LLM with coder instructions, returns improved code.</li> <li><code>handle_result</code>: Called at the end. Calls LLM to rate the coder's skills based on the history and compares the final code with the initial code.</li> </ul> </li> <li>Define Conditional Logic (<code>deployment_ready</code>): A function that calls an LLM (<code>classify_feedback</code>) to determine if the reviewer's feedback has been addressed in the latest code. Also checks iteration count.</li> <li>Build Graph:<ul> <li>Adds nodes.</li> <li>Sets entry point (<code>handle_reviewer</code>).</li> <li>Adds edges (<code>handle_coder</code> -&gt; <code>handle_reviewer</code>, <code>handle_result</code> -&gt; END).</li> <li>Adds conditional edge from <code>handle_reviewer</code> based on <code>deployment_ready</code> function (either to <code>handle_coder</code> for more revisions or <code>handle_result</code> to finish).</li> </ul> </li> <li>Compile &amp; Invoke: <code>workflow.compile()</code> creates the runnable graph. <code>app.invoke(...)</code> starts the process with the initial objective, code, and state.</li> </ul> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/07-multi-agent-collaboration\n# Ensure langgraph, langchain-openai are installed\npython coding-agents.py\n</code></pre> <p>Observe the output. You'll see the Coder generating initial code, the Reviewer providing feedback, the Coder revising, and the cycle repeating until the <code>deployment_ready</code> condition is met (either feedback addressed or iteration limit exceeded), finally ending with the <code>handle_result</code> node.</p> <p>LangGraph for Structured Workflows</p> <p>LangGraph excels at defining explicit, complex workflows involving multiple agents or steps with clear transitions and conditional logic. It provides more control over the interaction pattern compared to more free-form agent frameworks.</p>"},{"location":"day3/module7/multi_agent/#2-reasoning-based-approach-single-reasoning-coder-autogenmagenticone","title":"2. Reasoning-Based Approach: Single Reasoning Coder (AutoGen/MagenticOne)","text":"<p>File: <code>src/07-multi-agent-collaboration/reasoning-coder.py</code></p> <p>This script contrasts the multi-agent approach by using a single, powerful reasoning agent (specifically targeting <code>o1-mini</code> via <code>autogen-ext</code>) to handle the coding task directly.</p> <p>Concept:</p> <p>Instead of breaking the task into explicit Coder and Reviewer roles managed by a graph, this approach relies on the advanced reasoning capabilities of the <code>o1-mini</code> model to understand the request and generate the code in one go (or through internal reasoning steps not exposed externally in this simple script).</p> <ul> <li>Agent: An <code>AssistantAgent</code> from <code>autogen_agentchat</code> is configured to use the <code>o1-mini</code> model via <code>OpenAIChatCompletionClient</code>.</li> <li>Direct Invocation: The <code>generate_code</code> function directly invokes the <code>reasoning_agent</code> with the coding task.</li> </ul> <p>Code Breakdown:</p> <ul> <li>Import Libraries: <code>autogen_agentchat</code>, <code>autogen_core</code>, <code>autogen_ext</code>, <code>asyncio</code>.</li> <li>Initialize Model Client: <code>OpenAIChatCompletionClient</code> configured for <code>o1-mini</code> at the GitHub Models endpoint.</li> <li>Define Reasoning Agent: <code>AssistantAgent</code> named <code>reasoning_agent</code> using the <code>o1_model_client</code>.</li> <li><code>generate_code</code> Function: An async function that takes the task description and calls <code>reasoning_agent.on_messages(...)</code> to get the code.</li> <li>Main Execution: Calls <code>generate_code</code> with the same query used in the LangGraph example and prints the result.</li> </ul> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/07-multi-agent-collaboration\n# Ensure autogen-agentchat, autogen-core, autogen-ext are installed\n# Note: `autogen-ext` is used here; ensure it is installed correctly, either as a custom extension within this repository or via separate installation steps if required.\npython reasoning-coder.py\n</code></pre> <p>Observe the output. It should directly print the generated code for the regression model task, produced by the single <code>o1-mini</code> agent.</p> <p>When to Use a Single Powerful Reasoner</p> <p>If a single, highly capable LLM (like <code>o1-mini</code> or GPT-4) can reliably handle the entire task through its own reasoning, and explicit intermediate steps or checks aren't strictly necessary, this approach can be simpler to implement.</p> <p>Comparison:</p> <ul> <li>LangGraph (Multi-Agent):<ul> <li>Pros: Explicit control over workflow, clear roles, better for complex processes requiring specific steps or checks, easier debugging of individual components.</li> <li>Cons: More setup required to define the graph, nodes, and transitions.</li> </ul> </li> <li>AutoGen/o1-mini (Single Reasoning Agent):<ul> <li>Pros: Simpler setup for certain tasks, leverages the advanced reasoning of the model.</li> <li>Cons: Less explicit control over the process, relies heavily on the model's ability to understand and execute the task correctly, debugging can be harder as the reasoning is internal to the LLM.</li> </ul> </li> </ul> <p>The choice between these approaches depends on the complexity of the task, the need for explicit control and intermediate checks, and the capabilities of the underlying LLMs.</p> <p>Further Reading &amp; Resources (Multi-Agent Collaboration)</p> <p>Explore frameworks and concepts for building multi-agent systems:</p> <ul> <li>Frameworks &amp; Libraries:<ul> <li>LangGraph Documentation: Multi-agent Collaboration</li> <li>crewAI Documentation (Popular framework for collaborative agents)</li> <li>Microsoft AutoGen Documentation (Framework for multi-agent conversation)</li> <li>Semantic Kernel Documentation: Agents (Includes concepts for agent collaboration)</li> </ul> </li> <li>Tutorials &amp; Guides:<ul> <li>How to Build the Ultimate AI Automation with Multi-Agent Collaboration (LangChain Blog)</li> <li>Building Multi AI Agent Systems: A Practical Guide! (LinkedIn)</li> <li>Step by Step guide to develop AI Multi-Agent system using Microsoft Semantic Kernel (Medium)</li> <li>How to Build a Multi Agent AI System (YouTube - IBM)</li> <li>Build a Multi-Agent System with CrewAI | Agentic AI Tutorial (YouTube)</li> </ul> </li> <li>Courses:<ul> <li>Multi AI Agent Systems with crewAI (DeepLearning.AI)</li> </ul> </li> </ul> <p>This module introduced multi-agent collaboration using structured workflows (LangGraph) and contrasted it with a single powerful reasoning agent. The next module will explore more dynamic multi-agent interactions and hierarchical structures using frameworks like AutoGen.</p>"},{"location":"day3/module7/solutions/","title":"Day 3 - Module 7: Solutions","text":"<p>These are the solutions or discussion points for the exercises in Module 7.</p>"},{"location":"day3/module7/solutions/#solution-71-changing-the-coding-task-multi-agent","title":"Solution 7.1: Changing the Coding Task (Multi-Agent)","text":"<p>Modify <code>src/07-multi-agent-collaboration/coding-agents.py</code>:</p> <pre><code># ... (imports, state, nodes, graph definition remain the same) ...\n\nif __name__ == \"__main__\":\n    # ... (LLM setup) ...\n\n    # --- Modified Query --- \n    query = \"Write a Python function that takes a list of numbers and returns the sum of squares.\"\n    # --------------------\n\n    # Initial code (can be empty or a basic structure)\n    initial_code = \"\"\"\ndef sum_of_squares(numbers):\n    # TODO: Implement function\n    pass\n\"\"\"\n\n    # ... (workflow compilation) ...\n\n    # Invoke the graph\n    inputs = GraphState(\n        objective=query,\n        code=initial_code,\n        feedback=\"\",\n        history=[],\n        iteration=0,\n        specialization=\"python\",\n        final_code=\"\",\n        rating=\"\"\n    )\n\n    # ... (invocation and printing results) ...\n</code></pre> <p>Expected Output: The execution trace should show the Coder agent attempting to implement the <code>sum_of_squares</code> function. The Reviewer provides feedback on correctness (e.g., using a loop or list comprehension), handling empty lists, or style. The process aims to converge to a correct implementation like:</p> <pre><code>def sum_of_squares(numbers):\n    \"\"\"Calculates the sum of the squares of numbers in a list.\"\"\"\n    if not isinstance(numbers, list):\n        raise TypeError(\"Input must be a list of numbers.\")\n    return sum(x**2 for x in numbers)\n</code></pre>"},{"location":"day3/module7/solutions/#solution-72-adjusting-review-criteria-multi-agent","title":"Solution 7.2: Adjusting Review Criteria (Multi-Agent)","text":"<p>Modify <code>src/07-multi-agent-collaboration/coding-agents.py</code>:</p> <pre><code># ... (imports, state, nodes, graph definition remain the same) ...\n\n# --- Modified Reviewer Prompt --- \nreviewer_start = PromptTemplate.from_template(\n    \"\"\"You are a senior software engineer acting as a reviewer. Your goal is to ensure the code is production-ready.\\\n    Review the code provided based on the objective: {objective}.\\\n    Provide constructive feedback to the coder, focusing on correctness, style (PEP8), efficiency, and potential bugs.\\\n    **Ensure that all functions include type hints.** \\\n    If the code is satisfactory and meets all criteria, respond with 'Code approved'. Otherwise, provide specific feedback for improvement.\"\n    \"\"\"\n)\n# ------------------------------\n\n# ... (rest of the script, including node definitions using the prompt) ...\n</code></pre> <p>Expected Output: When running with this modified prompt, the Reviewer agent should now explicitly check for type hints in the code generated by the Coder. If the Coder submits code like <code>def sum_of_squares(numbers):</code>, the Reviewer's feedback should include a point about missing type hints (e.g., \"Feedback: Add type hints to the function signature (e.g., <code>numbers: list[int]</code>) and return type (e.g., <code>-&gt; int</code>).\"). This will force the Coder to add type hints in the subsequent revision.</p>"},{"location":"day3/module7/solutions/#solution-73-comparing-single-vs-multi-agent-output","title":"Solution 7.3: Comparing Single vs. Multi-Agent Output","text":"<p>This requires running both scripts with the same query and comparing the results manually.</p> <p>Example Comparison (Task: Sum of Squares function):</p> <ul> <li><code>coding-agents.py</code> (Multi-Agent): Produces well-commented, type-hinted code with basic error handling (like checking for list input) after a few review cycles. <code>reasoning-coder.py</code> (Single Agent - o1-mini):* Produces a functional implementation quickly, but often without comments, type hints, or extensive error handling unless specifically prompted. The quality depends heavily on the <code>o1-mini</code> model\\'s training and reasoning ability for coding tasks.</li> </ul> <p>Discussion Points:*   The multi-agent approach allows for enforcing specific standards (via the reviewer) that are not inherent in the single agent\\\\'s default behavior. The single reasoning agent is faster for simpler tasks if its baseline quality is high. *   The multi-agent system provides more transparency into the refinement process (seeing the feedback cycles).</p>"},{"location":"day3/module7/solutions/#solution-74-conceptual-designing-a-trip-planning-graph","title":"Solution 7.4 (Conceptual): Designing a Trip Planning Graph","text":"<p>This involves sketching a potential graph structure.</p> <p>Potential <code>GraphState</code>:</p> <pre><code>class TripState(TypedDict):\n    origin: str\n    destination: str\n    start_date: str\n    end_date: str\n    budget: Optional[float]\n    interests: Optional[list[str]]\n    flight_options: Optional[list[dict]]\n    hotel_options: Optional[list[dict]]\n    activity_options: Optional[list[dict]]\n    chosen_flight: Optional[dict]\n    chosen_hotel: Optional[dict]\n    chosen_activities: Optional[list[dict]]\n    itinerary: Optional[str]\n    user_feedback: Optional[str]\n    last_agent: str # To track who spoke last for conditional logic\n</code></pre> <p>Potential Workflow Sketch:</p> <ol> <li>Entry Point: <code>GetUserPrefs</code> (Node using <code>UserInteraction</code> agent, possibly interrupting to get destination, dates, budget, interests). -&gt; Updates state.</li> <li>Parallel Search: Edges from <code>GetUserPrefs</code> to <code>FindFlights</code> (Node using <code>FlightFinder</code>), <code>FindHotels</code> (Node using <code>HotelFinder</code>), <code>FindActivities</code> (Node using <code>ActivityPlanner</code>). These run in parallel.</li> <li>Present Options: Edges from search nodes to <code>PresentOptions</code> (Node using <code>UserInteraction</code>). This node formats the found options.</li> <li>Interrupt for User Choice: <code>PresentOptions</code> interrupts, showing flight, hotel, and activity options, asking the user to choose or provide feedback. -&gt; Updates state with <code>user_feedback</code> or <code>chosen_flight</code>, <code>chosen_hotel</code>, etc.</li> <li>Conditional Edge: Based on <code>user_feedback</code>:<ul> <li>If user chose options: Go to <code>CompileItinerary</code>.</li> <li>If user asked for refinement (e.g., \"cheaper hotels\"): Go back to <code>FindHotels</code> (passing <code>user_feedback</code>).</li> <li>If user wants different destination: Go back to <code>GetUserPrefs</code>.</li> </ul> </li> <li><code>CompileItinerary</code>: (Node using <code>ItineraryCompiler</code>) Takes chosen flight, hotel, activities and creates a structured itinerary. -&gt; Updates <code>itinerary</code> in state.</li> <li><code>PresentFinalItinerary</code>: (Node using <code>UserInteraction</code>) Shows the final itinerary to the user.</li> <li>END</li> </ol> <p>This sketch highlights the use of parallel execution, interrupts for user input, and conditional branching based on that input, which are key features of LangGraph for building complex, interactive multi-agent systems.</p>"},{"location":"day3/module8/exercises/","title":"Day 3 - Module 8: Exercises","text":"<p>These exercises are based on the concepts and code presented in Module 8: Society of Agents (AutoGen/MagenticOne) (<code>src/08-society-of-agents/</code>).</p>"},{"location":"day3/module8/exercises/#exercise-81-changing-the-task-simple-group","title":"Exercise 8.1: Changing the Task (Simple Group)","text":"<p>Goal: Observe how the MagenticOne orchestrator directs different agents based on the initial task.</p> <ol> <li>Open the <code>src/08-society-of-agents/simple-group.py</code> script.</li> <li>Locate the line where the chat is initiated:     <pre><code>stream = magenticteam.run_stream(task=\"what time is it here?.\")\n</code></pre></li> <li>Change the initial <code>task</code> string to ask for different information that requires different agents. Examples:<ul> <li><code>task=\"Where does Dennis live?\"</code> (Should primarily involve <code>users_agent</code> and <code>location_agent</code>)</li> <li><code>task=\"Who is the current user?\"</code> (Should primarily involve <code>users_agent</code>)</li> </ul> </li> <li>Run the script (<code>python simple-group.py</code>).</li> <li>Observe the conversation flow printed to the console. Does the orchestrator correctly involve the agents relevant to the new task? Does the <code>summary_agent</code> still get involved appropriately?</li> </ol>"},{"location":"day3/module8/exercises/#exercise-82-adding-a-weather-agent-simple-group","title":"Exercise 8.2: Adding a Weather Agent (Simple Group)","text":"<p>Goal: Practice adding a new agent with a specific capability to the group chat.</p> <ol> <li>Open the <code>src/08-society-of-agents/simple-group.py</code> script.</li> <li>Define a weather tool: Add a <code>get_weather(location: str)</code> function (you can make it return a fixed string like \"It is sunny in [location]\" for simplicity).</li> <li>Define a weather agent: Create a new <code>AssistantAgent</code> named <code>weather_agent</code>.<ul> <li>Give it a relevant <code>description</code> (e.g., \"Knows the weather.\").</li> <li>Provide it with the <code>get_weather</code> tool function.</li> <li>Use the same <code>model_client</code> as the other agents.</li> </ul> </li> <li>Add agent to group: Include the <code>weather_agent</code> in the <code>agents</code> list passed to <code>MagenticOneGroupChat</code>.</li> <li>Modify the task: Change the initial <code>task</code> to include asking about the weather, e.g., <code>task=\"What is the weather like where Dennis lives?\"</code>.</li> <li>Run the script (<code>python simple-group.py</code>).</li> <li>Observe the conversation. Does the orchestrator involve the new <code>weather_agent</code> after finding Dennis's location? Does the final summary include the weather information?</li> </ol>"},{"location":"day3/module8/exercises/#exercise-83-testing-chef-agent-logic-chef-group","title":"Exercise 8.3: Testing Chef Agent Logic (Chef Group)","text":"<p>Goal: Verify if the chef agent correctly handles missing information (allergies) based on its instructions.</p> <ol> <li>Open the <code>src/08-society-of-agents/chef-and-group.py</code> script.</li> <li>Locate the <code>get_medical_history</code> tool function.</li> <li>Modify the function to return an empty string or a message indicating no known allergies:     <pre><code>def get_medical_history(username: str) -&gt; str:\n    \"\"\"Returns the medical history for a given username.\"\"\"\n    if username == \"Dennis\":\n        # return \"Dennis is allergic to peanuts.\"\n        return \"No known allergies for Dennis.\"\n    else:\n        return \"Unknown user.\"\n</code></pre></li> <li>Run the script (<code>python chef-and-group.py</code>) with the task \"I want to have something to eat. What would you recommend?.\"</li> <li>Carefully read the conversation flow. Does the <code>chef_agent</code>, after finding out there are no known allergies from the <code>users_agent</code>, explicitly ask something like \"Do you have any allergies I should be aware of?\" before recommending a dish? (This depends on its system prompt instructions being followed correctly by the LLM).</li> </ol>"},{"location":"day3/module8/exercises/#exercise-84-adding-constraints-and-preferences-into-a-planning-agent","title":"Exercise 8.4: Adding Constraints and preferences into a planning agent","text":"<p>Goal: Understand how a group of agents can collaborate without a moderator. Learn how context agents (location, user preferences) feed into a planning agent (chef). Work with a real agent configuration and extend it with constraints. </p> <ol> <li> <p>Add Domain Expertise (10\u201315 min):</p> </li> <li> <p>Add a new agent: <code>nutritionist_agent</code>.</p> <ul> <li>Role: Evaluate the nutritional quality of the suggested meal.</li> <li>Provide it a tool: <code>analyze_nutrition(meal: str)</code> \u2014 mock this as a function returning \u201cbalanced\u201d, \u201chigh-fat\u201d, etc.</li> </ul> </li> <li> <p>Inject a Constraint (5 min):</p> </li> <li> <p>Modify <code>users_agent</code> to include a dietary preference, e.g., <code>\"low-carb\"</code> or <code>\"vegan\"</code>.</p> </li> <li> <p>Ensure the <code>chef_agent</code> respects this constraint when generating meal options.</p> </li> <li> <p>Optional Extensions:</p> </li> <li> <p>Add a <code>budget_agent</code> with a <code>get_cost_estimate(meal)</code> tool.</p> </li> <li>Let agents negotiate (by reasoning in messages) over trade-offs between cost and nutrition.</li> </ol>"},{"location":"day3/module8/exercises/#exercise-85-modifying-reasoning-oversight-o1-group","title":"Exercise 8.5: Modifying Reasoning Oversight (o1 Group)","text":"<p>Goal: Change the focus of the quality check performed by the reasoning agent.</p> <ol> <li>Open the <code>src/08-society-of-agents/o1-with-chef-group.py</code> script.</li> <li>Locate the <code>check_conversation</code> async function (which acts as a tool for the <code>consultation_agent</code>).</li> <li>Find the prompt string passed to <code>reasoning_agent.on_messages</code> inside this function:     <pre><code>prompt = \"Check the messages for inconsistencies or open questions...\"\n</code></pre></li> <li>Modify this <code>prompt</code> to ask the <code>reasoning_agent</code> (o1-mini) to check for something specific. Examples:<ul> <li><code>prompt = \"Review the conversation. Did the chef confirm the user's allergies before recommending a meal? Answer yes or no and explain briefly.\"</code></li> <li><code>prompt = \"Analyze the conversation. Was the final meal recommendation appropriate given the available ingredients mentioned earlier?\"</code></li> </ul> </li> <li>Run the script (<code>python o1-with-chef-group.py</code>).</li> <li>Observe the output from the <code>consultation_agent</code> (which includes the response from the <code>reasoning_agent</code>). Does the feedback now focus specifically on the new check you asked the <code>reasoning_agent</code> to perform?</li> </ol>"},{"location":"day3/module8/society/","title":"Day 3 - Module 8: Society of Agents (AutoGen/MagenticOne)","text":"<p>Objective: Explore dynamic multi-agent conversations and collaboration patterns using frameworks like AutoGen, specifically the MagenticOne group chat implementation.</p> <p>Source Code: <code>src/08-society-of-agents/</code></p>"},{"location":"day3/module8/society/#introduction","title":"Introduction","text":"<p>While structured workflows like LangGraph (Module 7) provide explicit control, some scenarios benefit from more dynamic interactions where agents converse more freely, deciding who speaks next based on the conversation flow or an orchestrator. Frameworks like AutoGen (and its MagenticOne group chat implementation used here) excel at simulating these \"societies\" of agents.</p> <p>Key Concepts (AutoGen/MagenticOne):</p> <ul> <li>Agents (<code>AssistantAgent</code>): Define individual agents with specific roles, instructions (system messages), and tools.</li> <li>Group Chat (<code>MagenticOneGroupChat</code>): Manages the conversation between multiple agents.<ul> <li>Orchestrator: MagenticOne uses an underlying LLM (often a powerful reasoning model like <code>o1-mini</code> or <code>gpt-4o</code>) as an orchestrator. This orchestrator analyzes the conversation history and the agents' capabilities (descriptions, tools) to decide which agent should speak next to best advance the task.</li> <li>Dynamic Turns: Unlike predefined sequences or simple round-robin, the orchestrator dynamically selects the next speaker.</li> </ul> </li> <li>Task Execution: The group chat is initiated with a task, and the agents collaborate through conversation, managed by the orchestrator, until a termination condition is met.</li> <li>Termination: Conditions like <code>MaxMessageTermination</code> (limit number of messages) or <code>TextMentionTermination</code> (stop when a specific keyword like \"TERMINATE\" is mentioned) are used to end the chat.</li> </ul> <p>AutoGen &amp; MagenticOne</p> <p>AutoGen is a framework for building multi-agent applications. MagenticOne is a specific group chat implementation within or compatible with AutoGen, leveraging an LLM orchestrator for dynamic turn-taking based on agent descriptions and conversation context.</p> <p>This module explores variations of MagenticOne group chats:</p> <ol> <li>Simple Group: Multiple specialized agents collaborating on a task.</li> <li>Chef Group: A more complex scenario involving agents with overlapping and dependent knowledge (user info, location, time, ingredients, allergies) to recommend a meal.</li> <li>Group with Reasoning Oversight: Adding a dedicated reasoning agent (<code>o1-mini</code>) to monitor and provide feedback on the group chat quality.</li> </ol>"},{"location":"day3/module8/society/#1-simple-group-chat","title":"1. Simple Group Chat","text":"<p>File: <code>src/08-society-of-agents/simple-group.py</code></p> <p>This script sets up a basic MagenticOne group chat with agents specialized in getting user information, location, and time.</p> <p>Code Breakdown:</p> <ul> <li>Import Libraries: <code>autogen_agentchat</code>, <code>autogen_ext</code>, <code>asyncio</code>.</li> <li>Initialize Model Client: <code>OpenAIChatCompletionClient</code> for the agents (using <code>gpt-4o</code> in this example).</li> <li>Define Tools: Standard Python functions for <code>get_current_username</code>, <code>get_current_location_of_user</code>, <code>get_current_time</code>.</li> <li>Define Agents: Multiple <code>AssistantAgent</code> instances:<ul> <li><code>users_agent</code>: Knows username, has <code>get_current_username</code> tool.</li> <li><code>location_agent</code>: Knows location, has <code>get_current_location_of_user</code> tool.</li> <li><code>time_agent</code>: Knows time, has <code>get_current_time</code> tool.</li> <li><code>summary_agent</code>: Tasked with summarizing and concluding, instructed to use other agents and respond with \"TERMINATE\" when done.</li> <li>Each agent has a <code>description</code> which is crucial for the MagenticOne orchestrator to understand its capabilities.</li> </ul> </li> <li>Create Group Chat: <code>MagenticOneGroupChat(...)</code>:<ul> <li>Takes the list of participating agents.</li> <li>Uses an underlying <code>model_client</code> (for the orchestrator LLM).</li> <li>Sets a <code>termination_condition</code> (<code>MaxMessageTermination(5)</code>).</li> </ul> </li> <li>Run Chat: <code>magenticteam.run_stream(task=\"what time is it here?.\")</code> starts the chat with the initial task.</li> <li>Display Output: <code>Console(stream)</code> prints the conversation flow to the console.</li> </ul> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/08-society-of-agents\n# Ensure autogen-agentchat, autogen-ext, pytz are installed\npython simple-group.py\n</code></pre> <p>Observe the console output. You will see the agents conversing, involving the <code>users_agent</code>, <code>location_agent</code>, and <code>time_agent</code> being called upon by the orchestrator to gather the necessary information before the <code>summary_agent</code> provides the final answer or the chat hits the message limit.</p>"},{"location":"day3/module8/society/#2-chef-recommendation-group-chat","title":"2. Chef Recommendation Group Chat","text":"<p>File: <code>src/08-society-of-agents/chef-and-group.py</code></p> <p>This example presents a more complex scenario where agents need to collaborate to recommend a meal, considering user details, location, time, ingredients, and allergies.</p> <p>Code Breakdown:</p> <ul> <li>Similar Setup: Imports, model client, tools (adds <code>get_medical_history</code>, <code>get_available_incredients</code>).</li> <li>Agents:<ul> <li><code>users_agent</code>: Now also has <code>get_medical_history</code> tool.</li> <li><code>location_agent</code>, <code>time_agent</code>: Similar to the simple group.</li> <li><code>chef_agent</code>: Specialized in recommending dishes, has <code>get_available_incredients</code> tool, and importantly, its system message instructs it to ask about allergies if unknown.</li> <li><code>summary_agent</code>: Similar role.</li> </ul> </li> <li>Group Chat: <code>MagenticOneGroupChat</code> includes the <code>chef_agent</code> in the list.</li> <li>Task: <code>task=\"I want to have something to eat. What would you recommend?.\"</code></li> </ul> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/08-society-of-agents\npython chef-and-group.py\n</code></pre> <p>Observe the conversation. The orchestrator should guide the agents to gather necessary context: <code>users_agent</code> for username and allergies, <code>location_agent</code> for location, <code>time_agent</code> for time, and <code>chef_agent</code> for ingredients. The <code>chef_agent</code> should ideally ask about allergies if the <code>users_agent</code> didn't provide them, demonstrating rule-following and dynamic interaction based on missing information.</p>"},{"location":"day3/module8/society/#3-group-chat-with-reasoning-oversight-o1-mini","title":"3. Group Chat with Reasoning Oversight (o1-mini)","text":"<p>File: <code>src/08-society-of-agents/o1-with-chef-group.py</code></p> <p>This script enhances the chef scenario by adding a dedicated, powerful reasoning agent (<code>o1-mini</code>) to act as a consultant or quality checker for the main group chat.</p> <p>Code Breakdown:</p> <ul> <li>Multiple Model Clients: Initializes clients for both <code>gpt-4o-mini</code> (for regular agents) and <code>o1-mini</code> (for the reasoning agent).</li> <li>Reasoning Agent (<code>reasoning_agent</code>): An <code>AssistantAgent</code> configured to use the <code>o1_model_client</code>.</li> <li>Consultation Tool (<code>check_conversation</code>): An async function defined as a tool. When called, it invokes the <code>reasoning_agent</code> with the current conversation messages, asking it to check for inconsistencies or open questions.</li> <li>Consultation Agent (<code>consultation_agent</code>): A regular <code>AssistantAgent</code> (using <code>gpt-4o-mini</code>) whose only tool is <code>check_conversation</code>. Its system message tasks it with checking conversation quality.</li> <li>Agents: The list of agents for the <code>MagenticOneGroupChat</code> now includes the <code>consultation_agent</code> along with the others (user, location, time, chef).</li> <li>Group Chat &amp; Task: Similar setup to <code>chef-and-group.py</code>.</li> </ul> <p>To Run:</p> <pre><code>cd /home/ubuntu/agentic-playground/src/08-society-of-agents\n# Ensure necessary autogen libraries are installed\npython o1-with-chef-group.py\n</code></pre> <p>Observe the flow. The MagenticOne orchestrator dynamically decides to call the <code>consultation_agent</code> at certain points. When called, the <code>consultation_agent</code> uses its <code>check_conversation</code> tool, which in turn invokes the powerful <code>o1-mini</code> <code>reasoning_agent</code> to analyze the chat history and provide feedback. This demonstrates a hierarchical pattern where one agent leverages another, more capable agent for specific complex tasks like quality control.</p> <p>Hierarchical Agent Structures</p> <p>Integrating a powerful reasoning agent as a consultant or reviewer within a group of specialized agents is a common and effective pattern. It allows the specialized agents to handle routine tasks while the reasoning agent provides higher-level oversight, analysis, or complex decision-making.</p> <p>Further Reading &amp; Resources (Society of Agents / AutoGen)</p> <p>Delve deeper into AutoGen and dynamic multi-agent systems:</p> <ul> <li>AutoGen Documentation:<ul> <li>Microsoft AutoGen Homepage</li> <li>AutoGen Concepts: Agents</li> <li>AutoGen Concepts: Group Chat</li> </ul> </li> <li>Tutorials &amp; Examples:<ul> <li>AutoGen Tutorial: Automated Task Solving with Code Generation, Execution &amp; Debugging</li> <li>AutoGen Tutorial: Group Chat</li> <li>AutoGen Cookbook (Various examples)</li> <li>Building Multi-Agent Systems with AutoGen (YouTube - James Briggs)</li> </ul> </li> <li>Related Concepts:<ul> <li>AgentLite: Enabling Efficient Agent Interaction via Lite LLM (Discusses efficient orchestration)</li> </ul> </li> </ul> <p>This concludes the core modules of the workshop, covering the journey from basic LLM interaction to complex, collaborative multi-agent systems. Remember to explore the exercises for each module to solidify your understanding.</p>"},{"location":"day3/module8/solutions/","title":"Day 3 - Module 8: Solutions","text":"<p>These are the solutions or discussion points for the exercises in Module 8.</p>"},{"location":"day3/module8/solutions/#solution-81-changing-the-task-simple-group","title":"Solution 8.1: Changing the Task (Simple Group)","text":"<p>Modify <code>src/08-society-of-agents/simple-group.py</code>:</p> <pre><code># ... (imports, model client, tools, agents definition remain the same) ...\n\nif __name__ == \"__main__\":\n    # ... (group chat setup) ...\n\n    # --- Modified Task --- \n    task = \"Where does Dennis live?\"\n    # -------------------\n\n    stream = magenticteam.run_stream(task=task)\n    console = Console()\n    console.print(stream)\n</code></pre> <p>Expected Output: The conversation flow should primarily involve the <code>users_agent</code> (to confirm the user is Dennis) and the <code>location_agent</code> (to get the location for Dennis). The <code>time_agent</code> should not be involved. The <code>summary_agent</code> is called at the end to state the final answer (e.g., \"Dennis lives in Berlin.\") before terminating.</p>"},{"location":"day3/module8/solutions/#solution-82-adding-a-weather-agent-simple-group","title":"Solution 8.2: Adding a Weather Agent (Simple Group)","text":"<p>Modify <code>src/08-society-of-agents/simple-group.py</code>:</p> <pre><code># ... (imports, model client, other tools definitions remain the same) ...\n\n# --- Define New Weather Tool --- \ndef get_weather(location: str) -&gt; str:\n    \"\"\"Returns the current weather for a given location.\"\"\"\n    print(f\"Tool: Getting weather for {location}\")\n    # Simple mock response\n    return f\"It is currently sunny in {location}.\"\n# -----------------------------\n\n# ... (definitions for users_agent, location_agent, time_agent, summary_agent) ...\n\n# --- Define New Weather Agent --- \nweather_agent = AssistantAgent(\n    name=\"WeatherAgent\",\n    description=\"Knows the weather.\",\n    system_message=\"You provide weather information.\",\n    model_client=model_client,\n    tools=[get_weather]\n)\n# ------------------------------\n\nif __name__ == \"__main__\":\n    # --- Add weather_agent to the list --- \n    agents = [users_agent, location_agent, time_agent, summary_agent, weather_agent]\n    # -------------------------------------\n\n    magenticteam = MagenticOneGroupChat(\n        agents=agents,\n        model_client=model_client, # Orchestrator model\n        termination_condition=MaxMessageTermination(10) # Increased limit for more steps\n    )\n\n    # --- Modified Task --- \n    task = \"What is the weather like where Dennis lives?\"\n    # -------------------\n\n    stream = magenticteam.run_stream(task=task)\n    console = Console()\n    console.print(stream)\n</code></pre> <p>Expected Output: The conversation should involve: 1.  <code>users_agent</code> (confirming user is Dennis). 2.  <code>location_agent</code> (getting Dennis's location, e.g., Berlin). 3.  <code>weather_agent</code> (being called with the location \"Berlin\" to get the weather). 4.  <code>summary_agent</code> (providing the final answer, e.g., \"The weather where Dennis lives (Berlin) is currently sunny.\") before terminating.</p>"},{"location":"day3/module8/solutions/#solution-83-testing-chef-agent-logic-chef-group","title":"Solution 8.3: Testing Chef Agent Logic (Chef Group)","text":"<p>Modify <code>src/08-society-of-agents/chef-and-group.py</code>:</p> <pre><code># ... (imports, model client, other tools definitions remain the same) ...\n\n# --- Modified Medical History Tool --- \ndef get_medical_history(username: str) -&gt; str:\n    \"\"\"Returns the medical history for a given username.\"\"\"\n    if username == \"Dennis\":\n        # return \"Dennis is allergic to peanuts.\"\n        return \"No known allergies for Dennis.\"\n    else:\n        return \"Unknown user.\"\n# -----------------------------------\n\n# ... (agent definitions, group chat setup, task remain the same) ...\n</code></pre> <p>Expected Output: When the conversation reaches the <code>chef_agent</code>, having already learned the username is Dennis but that there are \"No known allergies\", the <code>chef_agent</code> asks a clarifying question based on its system prompt (which includes instructions like \"If allergies are unknown, ask the user\"). The output should show the <code>chef_agent</code> asking something like:</p> <p><pre><code>ChefAgent: Okay Dennis, I see no known allergies on file. To make sure I recommend something safe, do you have any food allergies or dietary restrictions I should be aware of?\n</code></pre> This demonstrates the agent following its instructions to proactively seek missing critical information.</p>"},{"location":"day3/module8/solutions/#exercise-84-adding-constraints-and-preferences-into-a-planning-agent","title":"Exercise 8.4: Adding Constraints and preferences into a planning agent","text":"<p>File: <code>chef-and-group.py</code> Concepts: Contextual agents, collaboration, basic tool use</p>"},{"location":"day3/module8/solutions/#expected-codechanges","title":"\ud83d\udd27 Expected Code/Changes","text":""},{"location":"day3/module8/solutions/#added-nutritionist_agent","title":"Added <code>nutritionist_agent</code>","text":"<pre><code>def analyze_nutrition(meal: str) -&gt; str:\n    if \"pasta\" in meal.lower():\n        return \"High in carbs.\"\n    if \"burger\" in meal.lower():\n        return \"High in fat\"\n    return \"Well-balanced.\"\n\nnutritionist_agent = AssistantAgent(\n    \"Nutritionist\",\n    model_client=model_client,\n    tools=[analyze_nutrition],\n    description=\"A helpful assistent that Evaluates the nutritional of meals.\",\n)\n</code></pre>"},{"location":"day3/module8/solutions/#updated-user_agent-with-dietary-constraint","title":"Updated user_agent with dietary constraint","text":"<pre><code>async def get_diet_preference(username: str) -&gt; str:\n    \"Get the diet preference for a given username.\"\n    print(\"executing get_diet_preference\")\n    return f\"{username} is a vegetarian and does not eat meat.\"\n</code></pre>"},{"location":"day3/module8/solutions/#updated-group-to-include-new-agent","title":"Updated group to include new agent","text":"<pre><code>group = MagenticGroupChat(\n    agents=[\n        user_agent,\n        location_agent,\n        time_agent,\n        chef_agent,\n        nutritionist_agent,\n        summary_agent\n    ]\n)\n</code></pre>"},{"location":"day3/module8/solutions/#solution-behaviorexpected-output","title":"Solution Behavior/Expected Output:","text":"<ul> <li>The <code>chef_agent</code> should now consider \"low-carb\" preferences and avoid recommending dishes like pasta.</li> <li>The <code>nutritionist_agent</code> should evaluate the dish, e.g.:</li> </ul> <p>\u201cThis dish is balanced and aligns with the user\u2019s low-carb goal.\u201d</p>"},{"location":"day3/module8/solutions/#solution-84-modifying-reasoning-oversight-o1-group","title":"Solution 8.4: Modifying Reasoning Oversight (o1 Group)","text":"<p>Modify <code>src/08-society-of-agents/o1-with-chef-group.py</code>:</p> <pre><code># ... (imports, model clients, tools, agent definitions remain the same) ...\n\n# Tool for consultation agent to check conversation quality\nasync def check_conversation(messages: list[dict[str, str]]) -&gt; str:\n    \"\"\"Checks the conversation for inconsistencies or open questions.\"\"\"\n    # --- Modified Prompt for Reasoning Agent --- \n    prompt = \"Review the conversation. Did the chef confirm the user's allergies before recommending a meal? Answer yes or no and explain briefly.\"\n    # -----------------------------------------\n    response = await reasoning_agent.on_messages(messages=[ChatMessage(role=\"user\", content=prompt)])\n    return response[0].content\n\n# ... (consultation_agent definition using check_conversation tool) ...\n# ... (group chat setup, task, main execution remain the same) ...\n</code></pre> <p>Expected Output: When the <code>consultation_agent</code> is invoked by the orchestrator, its output (which comes from the <code>reasoning_agent</code> via the <code>check_conversation</code> tool) should now specifically address whether the chef confirmed allergies. For example:</p> <ul> <li>If the chef did ask: <code>ConsultationAgent: Yes, the chef asked about allergies after learning none were on file before proceeding with the recommendation.</code></li> <li>If the chef did not ask (due to an LLM error or prompt issue): <code>ConsultationAgent: No, the chef did not explicitly confirm allergies with the user before making a recommendation, even though the initial medical history check showed no known allergies.</code></li> </ul> <p>This demonstrates tailoring the oversight provided by the reasoning agent to focus on specific quality aspects of the multi-agent interaction.</p>"},{"location":"intro/overview/","title":"Workshop Overview","text":"<p>Welcome to the AI Agent Workshop!</p> <p>This workshop uses the <code>agentic-playground</code> repository to introduce concepts and practical implementations of AI agents.</p> <p>Target Audience: This workshop is designed for developers, AI enthusiasts, and technical customers interested in understanding and building AI agents.</p> <p>Goal: By the end of this workshop, you will: *   Understand the fundamental concepts of AI agents. *   Explore various agent patterns (ReAct, Multi-Agent, Human-in-the-Loop). *   Gain hands-on experience with relevant frameworks and libraries used in the repository (e.g., OpenAI API patterns, LangChain, Semantic Kernel, AutoGen).</p> <p>Format: The workshop combines presentations, detailed code walkthroughs based on the repository examples, and hands-on exercises to reinforce learning.</p> <p>Repository: All code examples are sourced from the following GitHub repository: https://github.com/denniszielke/agentic-playground</p> <p>We encourage you to explore the repository alongside the workshop materials.</p> <p>Workshop Structure:</p> <ul> <li>Day 1: Foundations: Basic LLM interaction, handling multimodal inputs (images), and working with complex data structures.</li> <li>Day 2: Single Agents &amp; Orchestration: Equipping agents with tools, implementing autonomous agents (ReAct), and incorporating human-in-the-loop workflows.</li> <li>Day 3: Advanced Multi-Agent Systems: Designing systems with multiple collaborating agents using structured workflows and dynamic group chats.</li> </ul> <p>Conceptual Stack:</p> <p></p>"},{"location":"intro/setup/","title":"Setup &amp; Prerequisites","text":"<p>Before starting the hands-on exercises, please ensure you have the following setup completed.</p>"},{"location":"intro/setup/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>All code examples are sourced from the <code>agentic-playground</code> repository. Clone it to your local machine:</p> <pre><code>git clone https://github.com/denniszielke/agentic-playground.git\ncd agentic-playground\n</code></pre>"},{"location":"intro/setup/#2-python-environment","title":"2. Python Environment","text":"<p>We recommend using a virtual environment to manage dependencies.</p> <ul> <li>Ensure Python is installed: Python 3.9 or higher is recommended.</li> <li>Create a virtual environment: <pre><code># Windows\npython -m venv .venv\n.venv\\Scripts\\activate\n\n# macOS/Linux\npython3 -m venv .venv\nsource .venv/bin/activate\n</code></pre></li> <li>Install Dependencies: The repository includes a <code>requirements.txt</code> file listing the necessary Python packages.     <pre><code>pip install -r requirements.txt\n</code></pre> Note: Some examples require additional dependencies (e.g., <code>pytz</code>, <code>graphviz</code>, <code>requests</code>, <code>pyaudio</code>, <code>sounddevice</code>, <code>pydub</code>, <code>pyperclip</code>, specific <code>langchain</code>, <code>llama-index</code>, <code>semantic-kernel</code>, or <code>autogen</code> components). Install these as needed when running specific examples, following instructions in the module content or error messages. For Graphviz visualization (<code>knowledge-graphs.py</code>), you also need to install the Graphviz binaries separately:          *   macOS: <code>brew install graphviz</code>         *   Ubuntu/Debian: <code>sudo apt-get update &amp;&amp; sudo apt-get install -y graphviz</code>         *   Windows: Download from the official Graphviz website and add to PATH.</li> </ul>"},{"location":"intro/setup/#3-github-personal-access-token-pat","title":"3. GitHub Personal Access Token (PAT)","text":"<p>Many examples in this repository interact with LLMs hosted via GitHub Models inference endpoints. Accessing these requires a GitHub Personal Access Token (PAT).</p> <ul> <li>Generate a PAT:<ol> <li>Go to your GitHub Settings &gt; Developer settings &gt; Personal access tokens &gt; Tokens (classic).</li> <li>Click \"Generate new token\" (or \"Generate new token (classic)\").</li> <li>Give your token a descriptive name (e.g., \"Agentic Playground Workshop\").</li> <li>Set an expiration date.</li> <li>Crucially, for GitHub Models inference, a token with no specific scopes/permissions is sufficient.</li> <li>Click \"Generate token\".</li> <li>Copy the generated token immediately. You won't be able to see it again.</li> </ol> </li> <li>Configure the PAT: The scripts typically use the <code>python-dotenv</code> library to load environment variables from a <code>.env</code> file in the repository's root directory.<ol> <li>Create a file named <code>.env</code> in the root of the cloned <code>agentic-playground</code> directory.</li> <li>Add the following line, replacing <code>your_github_pat_here</code> with the token you just copied:     <pre><code>GITHUB_TOKEN=\"your_github_pat_here\"\n</code></pre></li> </ol> </li> </ul>"},{"location":"intro/setup/#4-key-libraries-overview-high-level","title":"4. Key Libraries Overview (High Level)","text":"<p>The repository utilizes several popular libraries for building AI agents:</p> <ul> <li>OpenAI Python Library (<code>openai</code>): Used for interacting with OpenAI-compatible APIs, including the GitHub Models endpoint. Provides methods for chat completions, streaming, tool calling, and multimodal inputs.</li> <li>LangChain (<code>langchain</code>, <code>langchain-openai</code>, etc.): A framework for developing applications powered by language models. Provides tools for managing prompts, chains, agents (like ReAct), memory, and document loading.</li> <li>LlamaIndex (<code>llama-index</code>, <code>llama-index-llms-openai</code>, etc.): A data framework for LLM applications, focusing on connecting LLMs with external data. Also provides agent implementations (like ReAct).</li> <li>Semantic Kernel (<code>semantic-kernel</code>): A Microsoft-developed SDK for integrating LLMs into applications. Offers features like plugins (tools), planners, memory, and agent abstractions (including process frameworks and agent chats).</li> <li>AutoGen (<code>autogen-agentchat</code>, <code>autogen-core</code>, <code>autogen-ext</code>): A framework for enabling multi-agent conversations and workflows, often using an orchestrator model to manage dynamic interactions.</li> <li>Pydantic: Used for data validation and defining structured data models (like the <code>KnowledgeGraph</code> or state objects).</li> </ul> <p>We will explore specific features of these libraries as we encounter them in the code examples throughout the workshop.</p>"}]}